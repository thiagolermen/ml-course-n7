{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgiKnAs1BbN"
      },
      "source": [
        "**Exercice Ré-identification**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cet exercice, on cherche à entraîner un réseau sur une tâche de ré-identification. On travaille ici avec un nombre restreint d'images réelles, provenant de caméras fixes différentes. Pour chaque caméra on dispose de dix images prises sous des météorologie/conditions d'éclairage différentes.\n",
        "\n",
        "Comme le jeu est de petite taille, on peut se permettre de le charger entièrement sur une carte GPU. C'est l'objet des cellules de codes qui suivent les imports:"
      ],
      "metadata": {
        "id": "rD7604MrjaP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU0zdFYCLdgR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torch import randint\n",
        "import os\n",
        "from os.path import join\n",
        "ls = lambda rep: sorted(os.listdir(rep))\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Liens valable jusqu'au 17/01/2024:\n",
        "# data à charger:\n",
        "! wget https://www.grosfichiers.com/L7R8MLxtfzG_NwC5CzNnTSt\n",
        "! tar xvf L7R8MLxtfzG_NwC5CzNnTSt\n",
        "! rm L7R8MLxtfzG_NwC5CzNnTSt\n",
        "# module à charger\n",
        "! wget https://www.grosfichiers.com/HAnmgiuVNGn_XtdcYCeMNnJ\n",
        "! mv HAnmgiuVNGn_XtdcYCeMNnJ utile_BE.py\n",
        "from utile_BE import *"
      ],
      "metadata": {
        "id": "lmLIpHHPm5Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Quelle sont les dimensions du tenseur contenu dans 'data/tensor_train.pt' ? A quoi ces dimensions correspondent-elles ?  "
      ],
      "metadata": {
        "id": "6xTeRSoRoLKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXhIC-7NZdq2"
      },
      "outputs": [],
      "source": [
        "# Les images sont empilées dans les .pt:\n",
        "tensor_train = torch.load('data/tensor_train.pt')\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Passer les trois tenseurs issus des .pt sur carte GPU. Les stocker dans le dictionnaire ds_tensor:"
      ],
      "metadata": {
        "id": "0DadJ9KdoqAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_tensor = {}\n",
        "# Les images sont empilées dans les .pt:\n",
        "ds_tensor['train'] = ...\n",
        "ds_tensor['val'] = ...\n",
        "ds_tensor['test'] = ..."
      ],
      "metadata": {
        "id": "3jSfVaM8o-RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfZ6Y7l0a5Ol"
      },
      "source": [
        "**Q3** Pour entraîner le réseau, on utilise les transformations de données définies dans la cellule suivante. Quels sont les rôles de self.rotate et TF.hflip dans *tr_triplet*?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt_ZRE43aMMp"
      },
      "outputs": [],
      "source": [
        "# Les transformations de données:\n",
        "\n",
        "class tr_triplet:\n",
        "\n",
        "    def __init__(self, marginsup, margininf, cropped_prop, size_in, size_out, zoom =20,\n",
        "                 rotation = False, angle_max= 5, perspective = False, ph = 0.2, pv = 0.05,  colorjitter=False, fixed_top = False, **kwargs):\n",
        "        self.marginsup = marginsup\n",
        "        self.margininf = margininf\n",
        "        self.size_out = size_out\n",
        "        self.size_in = size_in\n",
        "        self.ysup = round(self.marginsup * self.size_in)\n",
        "        self.yinf = round(self.margininf * self.size_in)\n",
        "        self.fixed_top = False\n",
        "        self.cropped_prop = cropped_prop\n",
        "        self.zoom = zoom\n",
        "        self.rotation = rotation\n",
        "        self.angle_max = angle_max\n",
        "        self.colorjitter = colorjitter\n",
        "        self.perspective = perspective\n",
        "        self.pv = pv\n",
        "        self.ph = ph\n",
        "\n",
        "\n",
        "        if self.colorjitter:\n",
        "            self.cj = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.1, hue=0.1)\n",
        "\n",
        "        self.rs_out = transforms.Resize(size_out)\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "\n",
        "        if self.rotation:\n",
        "            angle = torch.randint(-self.angle_max, self.angle_max, (1,)).item()\n",
        "\n",
        "        reduc = torch.randint(0,self.zoom,(1,)).item()/100\n",
        "\n",
        "        size_crop = round((1-reduc)*self.cropped_prop*self.size_in)\n",
        "\n",
        "        top = torch.randint(self.ysup,\n",
        "                            self.size_in - size_crop - self.yinf,(1,)).item()\n",
        "        if self.fixed_top:\n",
        "            top = self.ysup\n",
        "\n",
        "        left = torch.randint(0, self.size_in - size_crop,(1,)).item()\n",
        "        tr_imgs = []\n",
        "\n",
        "        if self.perspective:\n",
        "\n",
        "            htl = ( - self.ph*torch.rand(1).item())* self.size_in\n",
        "            vtl = ( - self.pv*torch.rand(1).item())* self.size_in\n",
        "\n",
        "            htr = (1+ self.ph* torch.rand(1).item())* self.size_in\n",
        "            vtr = ( - self.pv* torch.rand(1).item())* self.size_in\n",
        "\n",
        "            hbr = (1 + self.ph* torch.rand(1).item())* self.size_in\n",
        "            vbr = (1 + self.pv* torch.rand(1).item())* self.size_in\n",
        "\n",
        "            hbl = ( - self.ph* torch.rand(1).item())* self.size_in\n",
        "            vbl = (1 + self.pv*torch.rand(1).item())* self.size_in\n",
        "\n",
        "            startpoints =  [(0.,0.), (self.size_in,0.), (self.size_in,self.size_in),(0.,self.size_in)]\n",
        "            endpoints = [( htl , vtl ), ( htr , vtr), ( hbr , vbr ),( hbl , vbl )]\n",
        "\n",
        "        flip = torch.randint(0,2, (1,)).item()\n",
        "\n",
        "\n",
        "        for img in imgs:\n",
        "\n",
        "            if self.rotation:\n",
        "                img = TF.rotate(img, angle)\n",
        "\n",
        "            if self.perspective:\n",
        "                img =  TF.perspective(img,  startpoints, endpoints)\n",
        "\n",
        "            img = TF.crop(img,top,left,size_crop,size_crop)\n",
        "\n",
        "            if self.colorjitter:\n",
        "                img = self.cj(img)\n",
        "\n",
        "            img = self.rs_out(img)\n",
        "\n",
        "            if flip == 1:\n",
        "                img = TF.hflip(img)\n",
        "\n",
        "            img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
        "            tr_imgs.append(img)\n",
        "\n",
        "        return tr_imgs\n",
        "\n",
        "\n",
        "class Simple_crop_norsin:\n",
        "\n",
        "    def __init__(self, marginsup, margininf, cropped_prop, size_in, size_out, **kwargs):\n",
        "        self.marginsup = marginsup\n",
        "        self.margininf = margininf\n",
        "        self.size_out = size_out\n",
        "\n",
        "        self.size_in = size_in\n",
        "\n",
        "        self.ysup = round(self.marginsup * self.size_in)\n",
        "        self.yinf = round(self.margininf * self.size_in)\n",
        "\n",
        "        self.cropped_prop = cropped_prop\n",
        "        self.rs_out = transforms.Resize(size_out)\n",
        "\n",
        "\n",
        "    def __call__(self, imgs):\n",
        "\n",
        "        size_crop = round(self.cropped_prop * self.size_in)\n",
        "        top = round(self.ysup)\n",
        "        left = round(0.5*(self.size_in - size_crop))\n",
        "\n",
        "        tr_imgs = []\n",
        "        for img in imgs:\n",
        "            img = TF.crop(img,top,left,size_crop,size_crop)\n",
        "            img = self.rs_out(img)\n",
        "            img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
        "\n",
        "            tr_imgs.append(img)\n",
        "\n",
        "        return tr_imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34V7vOjGdPw9"
      },
      "outputs": [],
      "source": [
        "class DS_triplet_flat_pt(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset, tr = None): #dir_images, tr = None):\n",
        "        self.dataset = dataset\n",
        "        self.tr = tr\n",
        "        self.nb_of_images = self.dataset.shape[0] #len(self.images)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "\n",
        "        id_name_anchor =  randint(0, self.nb_of_images,(1,)).item()\n",
        "        id_cam_anchor =  id_name_anchor // 10\n",
        "        id_name_positive = 10*id_cam_anchor + randint(0,10,(1,)).item()\n",
        "        id_name_negative = randint(0, self.nb_of_images,(1,)).item()\n",
        "\n",
        "        anchor = self.dataset[id_name_anchor,:,:,:]\n",
        "        positive = self.dataset[id_name_positive,:,:,:]\n",
        "        negative = self.dataset[id_name_negative,:,:,:]\n",
        "\n",
        "        [anchor, positive, negative] = self.tr([anchor, positive, negative])\n",
        "\n",
        "        return anchor, positive, negative, id_name_anchor, id_name_positive, id_name_negative\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nb_of_images\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DS_singleton_flat(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset, tr = None): #dir_images, tr = None):\n",
        "        self.dataset = dataset\n",
        "        self.tr = tr\n",
        "        self.nb_of_images = self.dataset.shape[0] #len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "        img = self.dataset[idx,:,:,:]\n",
        "\n",
        "\n",
        "        return img, idx\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nb_of_images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Visualiser un batch du jeu d'entraînement. Préciser ce que sont les images \"ancre\", les images \"positive\", les images \"négative\"."
      ],
      "metadata": {
        "id": "zAdn0KDN14rr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUlm1OihdlLD"
      },
      "outputs": [],
      "source": [
        "size_in = 128\n",
        "size_out = 104\n",
        "\n",
        "kwargs = {'colorjitter':True,\n",
        "          'rotation':True,\n",
        "          'angle_max':5,\n",
        "          'perspective':True,\n",
        "          'ph':0.1,\n",
        "          'pv':0.1,\n",
        "          'zoom':20\n",
        "          }\n",
        "\n",
        "marginsup = 0.05\n",
        "margininf = 0.05\n",
        "cropped_prop = 0.8\n",
        "\n",
        "TSC = tr_triplet(marginsup, margininf, cropped_prop, size_in, size_out, **kwargs)\n",
        "SC = Simple_crop_norsin(0.1, 0.05, 0.85, size_in, size_out)\n",
        "\n",
        "tr = {'train': TSC,\n",
        "      'val':  TSC,\n",
        "      'test':  SC,\n",
        "      }\n",
        "\n",
        "ds = {phase : DS_triplet_flat_pt(ds_tensor[phase], tr[phase]) \\\n",
        "              for phase in ['train', 'val']}\n",
        "\n",
        "print('Dataset entraînement')\n",
        "print('nb de séquences homogènes :')\n",
        "print(ds['train'].__len__()//10)\n",
        "print(\"nb d'images : \")\n",
        "print(ds['train'].__len__())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3FzsJSfdqws"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    #Imshow for Tensor\n",
        "    inp = inp.cpu().detach().numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "dl_viz = {phase : torch.utils.data.DataLoader(ds[phase], shuffle = True, batch_size=8, num_workers=0)\\\n",
        "              for phase in ['train', 'val']}\n",
        "\n",
        "\n",
        "imga, imgb, imgc, namea, nameb, namec  = next(iter(dl_viz['train']))\n",
        "\n",
        "\n",
        "print(namea, nameb, namec)\n",
        "\n",
        "outa = utils.make_grid(imga)\n",
        "outb = utils.make_grid(imgb)\n",
        "outc = utils.make_grid(imgc)\n",
        "\n",
        "\n",
        "plt.figure(figsize = (20,2))\n",
        "imshow(outa)\n",
        "plt.figure(figsize = (20,2))\n",
        "imshow(outb)\n",
        "plt.figure(figsize = (20,2))\n",
        "imshow(outc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Instancier un modèle ResNet50 (utiliser torchvision.models.resnet50) pré-entraîné sur le jeu imagenet. Modifier la dernière couche du réseau pour une tâche de réapprentissage (on travaillera dans un espace de sortie de dimension 10)."
      ],
      "metadata": {
        "id": "9NhDKCSS2HAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "nclasses = ...\n",
        "model = ...\n",
        "num_ftrs = ...\n",
        "model.fc = ..."
      ],
      "metadata": {
        "id": "9lVJInjT3E5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Compléter la boucle d'apprentissage et vérifier son bon fonctionnement sur une époque."
      ],
      "metadata": {
        "id": "ncr8uySc3HhY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGSPffvWeYkd"
      },
      "outputs": [],
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def calc_euclidean(self, x1, x2):\n",
        "        return (x1 - x2).pow(2).sum(1)\n",
        "\n",
        "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "        distance_positive = self.calc_euclidean(anchor, positive)\n",
        "        distance_negative = self.calc_euclidean(anchor, negative)\n",
        "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "        return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04oAeSqAeh-l"
      },
      "outputs": [],
      "source": [
        "num_epochs = 1\n",
        "\n",
        "dl = {phase : torch.utils.data.DataLoader(ds[phase], shuffle = True, batch_size=16, num_workers=0)\\\n",
        "              for phase in ['train','val']}\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "model = model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 50, gamma = 0.2)\n",
        "tripletloss = TripletLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az8FJ_2JeoVN"
      },
      "outputs": [],
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "train_losses=[]\n",
        "val_losses=[]\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        # Iterate over data.\n",
        "\n",
        "        for imga, imgb, imgc, _, _, _ in dl[phase]:\n",
        "\n",
        "\n",
        "            ...\n",
        "\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "              ...\n",
        "\n",
        "\n",
        "\n",
        "            running_loss += loss.item() *  ...  .size(0)\n",
        "\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(ds[phase])\n",
        "\n",
        "\n",
        "        print('{} Loss: {:.4f}'.format(\n",
        "            phase, epoch_loss))\n",
        "\n",
        "\n",
        "\n",
        "        if phase == 'train':\n",
        "            train_losses.append(epoch_loss)\n",
        "        elif phase == 'val':\n",
        "            val_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6** Charger les poids d'un modèle entraîné sur 50 époques ('data/reid_resnet50_60ep.pt')."
      ],
      "metadata": {
        "id": "zBcttFG53T8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_wts = torch.load('data/reid_resnet50_60ep.pt')\n",
        "...\n"
      ],
      "metadata": {
        "id": "lP5UIuXPp-yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7** Présenter des cas de réussite et d'échec sur le jeu de test:\n",
        "- deux images qui sont difficiles à attribuer (à l'oeil) à la même scènes et pour lesquelles le modèle prédit deux points relativement proches dans l'espace latent.\n",
        "- deux images faciles à attribuer à des scènes différentes et qui sont considérées comme issues de la même caméra par le modèle.\n",
        "On s'appuiera sur les bouts de code suivants:"
      ],
      "metadata": {
        "id": "8gh_nH9a6ztX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6IFgHeBfXCd"
      },
      "outputs": [],
      "source": [
        "ds['test'] = DS_singleton_flat(ds_tensor['test'], tr['test'])\n",
        "dl['test'] = torch.utils.data.DataLoader(ds['test'], shuffle = False, batch_size=10, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Y771b4faK_"
      },
      "outputs": [],
      "source": [
        "from numpy.ma.core import ids\n",
        "model.eval()\n",
        "outputs = []\n",
        "idxs = []\n",
        "for img, idx in dl['test']:\n",
        "            img = img.cuda()\n",
        "            with torch.set_grad_enabled(False):\n",
        "                output = model(img)\n",
        "            outputs.append(output.cpu())\n",
        "            del img, output\n",
        "            torch.cuda.empty_cache()\n",
        "            idxs += list(idx)\n",
        "outputs = torch.cat(outputs, dim=0)\n",
        "outputs = outputs.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xrnOEL2fdN-"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "dist = distance_matrix(outputs, outputs)\n",
        "from matplotlib.pyplot import imshow\n",
        "plt.figure(2)\n",
        "t = 1.1\n",
        "#Visualisation de la matrice des distances entre N et N+v:\n",
        "N = 900\n",
        "v = 100\n",
        "imshow((dist)[N:N+v,N:N+v], vmin=0, vmax=1, cmap='jet')\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fdi1VZBHClV"
      },
      "outputs": [],
      "source": [
        "# Visualisation d'une partie des images (ici, les images d'indices 860 à 870)\n",
        "def imshow(inp, title=None):\n",
        "    #Imshow for Tensor\n",
        "    inp = inp.cpu().detach().numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "end_ds =  torch.utils.data.Subset(ds['test'], range(860,870))\n",
        "dl_viz = torch.utils.data.DataLoader(end_ds, shuffle = False, batch_size=5, num_workers=0)\\\n",
        "\n",
        "imga, namea  = next(iter(dl_viz))\n",
        "imgz = imga.cpu()\n",
        "print(namea)\n",
        "#% Make a grid from batch\n",
        "outa = utils.make_grid(imga)\n",
        "plt.figure(figsize = (20,2))\n",
        "imshow(outa)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}