{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hrd-WSf6I5Y"
   },
   "source": [
    "**Partie II**\n",
    "\n",
    "In this section, we focus on two other important aspects of deep learning: speeding up learning with GPU cards and the ability to use pretrained networks.\n",
    "\n",
    "To illustrate the first aspect, we will use the GPUs available under Google Colab. To do this, before starting to read the notebook, go to **Modifier**/**Modifier les param du notebook** and select a GPU.\n",
    "\n",
    "For the second aspect, we will work on a binary classification problem using a small dataset (\"hymenoptera_data\"). We will see the benefits of using a network that has already been trained on a larger dataset and a more general classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8-J_sK83SAA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uO9dWAHQ1qzx",
    "outputId": "de4d3723-724b-48bc-b46f-a5de82a30751"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\") # 0 is the index of the GPU\n",
    "  print(\"You are on GPU !\")\n",
    "else:\n",
    "  print('Change the runtime type to GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wndXhSn-6v0k"
   },
   "source": [
    "## **A.** Load and viz the Hymenoptera dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52pFZ3U367Kc",
    "outputId": "ac364acf-8618-46ef-c2c8-d80645725a98"
   },
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip -qq hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g30QPYvJ6usN",
    "outputId": "cd2c88c5-4cc7-4f66-cddc-204a245400d0"
   },
   "outputs": [],
   "source": [
    "dir_data = 'hymenoptera_data'\n",
    "print(os.listdir(dir_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp8Wb2n3oO19"
   },
   "source": [
    "The dataset is in a standard format, and we can manipulate it with a ready-to-use dataset object of the datasets.ImageFolder class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zB_qzRQvoDR-",
    "outputId": "20a4ba70-1860-4bfa-fcd0-94b03253cede"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(dir_data, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "print('Dataset sizs:' )\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WqaCTQPpM3v"
   },
   "source": [
    "Since the provided dataset is very small, we need to maximize its utility. We will produce new images through additional transformations that preserve the nature of the object (data augmentation). \\\\\n",
    "In the code, transforms.*RandomResizedCrop()*, *transforms.RandomHorizontalFlip()* and *transforms.RandomVerticalFlip()* apply horizontal or vertical axis symmetry with a probability of 1/2. Note that these transformations might not be suitable for other datasets like MNIST since the mirror image of a digit is generally not another digit. Therefore, transformations often need to be adapted to the nature of the dataset. \\\\\n",
    "Some images are presented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRxgSsnkpAZd"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    if ax is None:\n",
    "      plt.imshow(inp)\n",
    "      plt.title(title)\n",
    "    else:\n",
    "      ax.imshow(inp)\n",
    "      ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "FNUCzqp5IKtM",
    "outputId": "75112d36-3368-4494-dd44-10f042068a26"
   },
   "outputs": [],
   "source": [
    "def plot_batch(images, labels, class_names):\n",
    "    num_images = len(images)\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        axs[i].axis('off')\n",
    "        imshow(images[i],axs[i],class_names[labels[i]])\n",
    "    plt.show()\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "class_names = image_datasets['train'].classes\n",
    "# Assuming `inputs` is a batch of images and `classes` are the corresponding class labels\n",
    "plot_batch(inputs, classes, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ2dPsjxAVUX"
   },
   "source": [
    "## **B.** Using a Graphics Card:\n",
    "\n",
    "Let's start by checking if, for typical architectures like ResNet, using a GPU significantly improves computation time. We will begin by loading the lightest of the ResNet architectures and select an appropriate loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52_1BQUc-N9H"
   },
   "source": [
    "**Exercise**\n",
    "\n",
    "- Load an untrained ResNet18. How many total weights does it contain? Check [here](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html).\n",
    "\n",
    "- How many neurons does the last layer of the network have?\n",
    "\n",
    "- Modify the last layer of the classifier so that it has as many neurons as there are classes in hymenoptera_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "BDYyavo6fPGU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4585d46815116744ed1e0861bb726f65",
     "grade": false,
     "grade_id": "cell-eafd7da8c7e07793",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d6dc4d65-0a95-427f-b814-6664f4105146",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avzLu9A9kfQJ"
   },
   "source": [
    "**Note:** Batch normalization operations facilitate learning but won't be detailed here. In summary, they center/reduce each feature map over the batch and then renormalize. The two renormalization parameters (mean and standard deviation) are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "WBHeH2oDnLxF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "412b4de0aa8334faa14a622df424c5eb",
     "grade": false,
     "grade_id": "cell-b052148498e265c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "59456f33-92d2-489e-cd15-61c7c1fc0e02",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Neurons in the last layer\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "n503vva5hbX4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fb28964abb2a7e652816a59e9f717f6",
     "grade": false,
     "grade_id": "cell-e4c2bf578b6fb64b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "fb351ee1-80c3-4bc5-ba0b-b3725a9824c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modification of the last layer of the classifier\n",
    "def get_model(pretrained):\n",
    "  model = models.resnet18(pretrained=pretrained)\n",
    "  # YOUR CODE HERE\n",
    "  raise NotImplementedError()\n",
    "  return model\n",
    "\n",
    "get_model(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "3q1LpTVNEiBX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bfcda4ee548c799305f5b6fec91141a",
     "grade": true,
     "grade_id": "cell-67d8d83e708666bb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**Exercise:** Is there a softmax layer at the end of the ResNet architecture?\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFxO-wGjqpRv"
   },
   "source": [
    "Now, let's define the negative log-likelihood as the cost function. To compute the log-likelihood, we could add a LogSoftmax layer to the ResNet. Yet it's easier and more common to use a loss function that includes *LogSoftmax*. In this regard, in PyTorch,  *nn.CrossEntropyLoss* combines both *LogSoftmax* and *NLLLoss*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pbB5UV9qqgF"
   },
   "outputs": [],
   "source": [
    "loss_fn =  nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaU7AXKAKJS_"
   },
   "source": [
    "Finally, let's define a function that incorporates the training loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsUcIAbaCRD7"
   },
   "source": [
    "**Exercise:**\n",
    "- Complete the *train_model* function, which takes a model, a loss function, an optimizer, and a number of epochs as arguments.\n",
    "- Train the ResNet over one epoch with mini-batches of 64 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "mf97k7ze4x-E",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acdd28db71eb8766aa3e59026f4a1e02",
     "grade": false,
     "grade_id": "cell-939679088c690723",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, num_epochs=1):\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                # Weights are not updated during the validation phase\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # 1. zeroing grads\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # forward step (outputs = ...)\n",
    "                    # get the predicted classes (preds = ...)\n",
    "                    # loss computing (loss = ...)\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                    if phase == 'train':\n",
    "                        # backward step\n",
    "                        # update wieghts\n",
    "                        # YOUR CODE HERE\n",
    "                        raise NotImplementedError()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {100*epoch_acc:.2f}%')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "fafNCRzhKHiT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c090554301ceb2dc90266beb7005241c",
     "grade": false,
     "grade_id": "cell-12523a6bb2edeea7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "74112f86-60f6-44c6-8c6d-4f4fb2865050",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "model = get_model(pretrained=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Launch the training over 1 epoch:\n",
    "# model = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3JiouQJKk2A"
   },
   "source": [
    "With more than 10 million parameters, training a ResNet on a CPU is much slower than the networks in Part I. \\\\\n",
    "Let's repeat the same training using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1RzhRh4LQAA",
    "outputId": "66bd5407-0d7c-4c3d-e1b7-77965b7c59a0"
   },
   "outputs": [],
   "source": [
    "print(f'Runtime device :{device}')\n",
    "\n",
    "# Load the model to the GPU:\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ki5PIDPLw4Z"
   },
   "source": [
    "To load a torch.tensor on GPU, the syntax is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HUBH0MKMCN3",
    "outputId": "61d9e2a3-74f0-4625-fdd2-dd928f7cdaed"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(2,1,4,4)\n",
    "print(\"On CPU :\\n\",x)\n",
    "x = x.to(device)\n",
    "# Note: You can also use .cuda() without specifying the device name\n",
    "# but this method is not recommended especially in a multi-gpu environment\n",
    "print(\"On GPU :\\n\",x)\n",
    "\n",
    "# bring back the x tensor to the CPU RAM:\n",
    "x = x.to('cpu') # or x.cpu()\n",
    "print('Back to CPU:\\n',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfXJyAFlMRQx"
   },
   "source": [
    "**Exercice :**\n",
    "- Complete the fonction *train_model_gpu* to train the model on GPU.\n",
    "- Compare the CPU and GPU training times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "rHqBOaX3NMmp",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5267acdfbaeaca0a8c63ea3aa6e70d0b",
     "grade": false,
     "grade_id": "cell-2ae12fd65c2a172b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "50b82850-7910-4d76-afbf-34ac75afa63f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_gpu(model, loss_fn, optimizer, num_epochs=1):\n",
    "    # Record the starting time\n",
    "    since = time.time()\n",
    "    # Loop through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Iterate through training and validation phases\n",
    "        for phase in ['train', 'val']:\n",
    "            # Set the model to training mode during the training phase, and evaluation mode during validation\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Initialize counters for loss and correct predictions\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate through batches in the data loader\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Move inputs and labels to the specified device (GPU)\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "\n",
    "                # Zero the gradients in the optimizer (same as in train_model())\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                # Forward pass: compute model outputs and predictions (same as in train_model())\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                    # Backward pass and optimization step if in the training phase (same as in train_model())\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                # Update counters\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                acc = torch.sum(preds == labels.data)\n",
    "                # The 'acc' tensor is distributed across different parts of the GPU\n",
    "                # Gather the 'acc' tensor on the CPU before accumulation\n",
    "                # running_corrects += ...\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Calculate average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            # Print epoch statistics\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {100*epoch_acc:.2f}%')\n",
    "\n",
    "    # Calculate and print the total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(pretrained=False).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "model = train_model_gpu(model, loss_fn, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZoxMQl-CiUN"
   },
   "source": [
    "### **GPU**\n",
    "A GPU, or Graphics Processing Unit, is a specialized electronic circuit designed to accelerate the processing of images and videos. Originally developed for rendering graphics in video games, GPUs have evolved into highly parallel processors that excel at performing many simultaneous calculations. Unlike CPUs (Central Processing Unit), the GPUs are optimized for parallel processing tasks.\n",
    "\n",
    "Key characteristics of GPUs include:\n",
    "\n",
    "1. **Parallel Processing:** GPUs consist of a large number of cores, allowing them to handle multiple tasks simultaneously. This makes them well-suited for parallelizable computations.\n",
    "\n",
    "2. **Vectorized Processing:** GPUs are designed to efficiently perform operations on vectors and matrices. This is particularly beneficial for tasks such as linear algebra operations, which are common in machine learning and scientific computing.\n",
    "\n",
    "3. **High Memory Bandwidth:** GPUs have high-speed memory to quickly access and manipulate large amounts of data, making them suitable for tasks that involve substantial data throughput.\n",
    "\n",
    "4. **Floating-Point Performance:** GPUs are optimized for floating-point arithmetic, which is important for applications that require precision in numerical calculations.\n",
    "\n",
    "5. **Specialized Compute APIs:** Modern GPUs support specialized compute APIs (Application Programming Interfaces) such as CUDA (Compute Unified Device Architecture) for NVIDIA GPUs or OpenCL (Open Computing Language), which allows developers to offload parallelizable tasks to the GPU.\n",
    "\n",
    "6. **General-Purpose GPU Computing:** Beyond graphics rendering, GPUs are now widely used for general-purpose computing, including scientific simulations, machine learning, data analysis, and other computationally intensive tasks.\n",
    "\n",
    "In the context of machine learning, deep learning frameworks like TensorFlow and PyTorch leverage GPUs to accelerate the training of neural networks. The parallel processing capabilities of GPUs enable the simultaneous computation of numerous mathematical operations, significantly speeding up training times compared to using only a CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPiZpob4xH7P"
   },
   "source": [
    "## **C.**  Improving the Training Procedure:\n",
    "\n",
    "Before comparing different transfer approaches, let's improve the training procedure a bit. We can add momentum to the gradient descent. The  [increment calculation](https://pytorch.org/docs/master/optim.html#torch.optim.SGD) will depend not only on the current gradient but also on past increment values stored in $d_i$:\n",
    "\n",
    "\\begin{equation}\n",
    "d_i^{t+1} = momentum \\times d_i^{t} +    \\dfrac{\\partial \\mathcal{L_{batch}^{t+1}}} {\\partial{\\omega_i}} \\\\\n",
    "w_i^{t+1}  = w_i^{t} - lr \\times d_i^{t+1}\n",
    "\\end{equation}\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "`optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zXmy-HFvWQR"
   },
   "source": [
    "Next, we'll add a *scheduler* that gradually decreases the learning rate *lr*. With the following *scheduler*, every five epochs, *lr* is multiplied by gamma = 0.1. \\\\\n",
    "`scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)`\n",
    "\n",
    "The *scheduler* needs access to the *lr* in the optimizer, so it is passed as an argument. The scheduler acts on *lr* only at the end of an epoch, and to make this action effective, you need to add this line outside the loader iteration loop:\n",
    "\n",
    "`scheduler.step()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DErP0Z3JvpEO"
   },
   "source": [
    "**Exercise**: Integrate these elements into the training function. Check that the learning rate decreases as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "CEaDuJ8qci4K",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9898958718e44ce28a614722ade8bb4",
     "grade": false,
     "grade_id": "cell-1edb88b91f8b8c6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_gpu_scheduler(model, loss_fn, optimizer,scheduler, num_epochs=1):\n",
    "    # Record the starting time\n",
    "    since = time.time()\n",
    "    # Loop through epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Iterate through training and validation phases\n",
    "        for phase in ['train', 'val']:\n",
    "            # Set the model to training mode during the training phase, and evaluation mode during validation\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            # Initialize counters for loss and correct predictions\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate through batches in the data loader\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Move inputs and labels to the specified device (same as in train_model_gpu())\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "\n",
    "                # Zero the gradients in the optimizer (same as in train_model())\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                # Forward pass: compute model outputs and predictions (same as in train_model())\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                    # Backward pass and optimization step if in the training phase (same as in train_model())\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "\n",
    "                # Update counters\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                acc = torch.sum(preds == labels.data)\n",
    "                # The 'acc' tensor is distributed across different parts of the GPU\n",
    "                # Gather the 'acc' tensor on the CPU before accumulation (same as in train_model_gpu())\n",
    "                # running_corrects += ...\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            # Calculate average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            # Print epoch statistics\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {100*epoch_acc:.2f}%')\n",
    "        # update the learning rate\n",
    "        # print it (hint: look at optimizer.param_groups)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # Calculate and print the total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "435lCrmPYLc8",
    "outputId": "b9425f4f-0e3a-405f-8fc1-402b0dc89096"
   },
   "outputs": [],
   "source": [
    "model = get_model(pretrained=False).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "\n",
    "model = train_model_gpu_scheduler(model, loss_fn, optimizer, scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T--j72IXvij_"
   },
   "source": [
    "**Important note:** As you may notice, there are quite a few parameters to set, `learning rate`, `momentum`, `gamma`, `step_size`, etc... Typically, we use the parameterization that achieves the best scores on the validation dataset. To do this, we save the weights during training as soon as a validation record is reached. Note that by selecting the model this way, we are essentially training on the validation dataset. \\\\\n",
    "This is why evaluating a selected model on the validation set always takes place on an independent **test dataset**, separate from the validation and training sets.\n",
    "\n",
    "For reference, here's how to store model weights in PyTorch:\n",
    "```python\n",
    "import copy\n",
    "\n",
    "# At the end of each epoch, save the best model:\n",
    "if phase == \"val\" and epoch_acc > best_val_acc:\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# After training, before testing the model, load the best model:\n",
    "model.load_state_dict(best_model_wts)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA0YW6DKeIbD"
   },
   "source": [
    "## **D.** Impact of pretraining on performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6bycjSu1Lj_"
   },
   "source": [
    "Training is faster on a GPU, but it only leads to a very poor score, barely better than random chance. To improve performance, a simple idea is to use a network trained on a similar (or more general) task as a starting point for learning. Here, it works particularly well with networks trained on ImageNet, whose convolutional filters are already very rich.\n",
    "\n",
    "**Note:**\n",
    "This method is refered to as **fine-tuning** a **pretrained model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMrZLn_5eVOg"
   },
   "source": [
    "**Exercise**:\n",
    "Modify the training function to retrieve successive training and validation accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "u1lj01DYrtOi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03464bce021df930d1bd720c974f8eb0",
     "grade": false,
     "grade_id": "cell-fae8accf732dfe01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer,scheduler, num_epochs=1):\n",
    "    # Record the starting time\n",
    "    since = time.time()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return model, accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9uwx43rwFde"
   },
   "source": [
    "**Exercise:** Compare two ResNet18 trainings, one randomly initialized and the other pre-trained, using learning curves, over 25 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "bjWpSKA5QLXT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07035b2ac33e340cbd639dd6d2a714b4",
     "grade": false,
     "grade_id": "cell-ee462750685c4631",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "a09102a7-aa2f-4b38-96b3-b85b4e3022e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 25\n",
    "# Learning \"from scratch\" (random weights) :\n",
    "# get the model\n",
    "# Put the model on GPU\n",
    "# get the loss, optimize, the scheduler and starting the training\n",
    "# ...\n",
    "# resnet_scratch, accs_scratch = train(...)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "P4Gd2XDHq1zS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d7c283669a16e353aa3930810d9b5db",
     "grade": false,
     "grade_id": "cell-54882ed9e75f5e07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "fce8d1f7-712b-4f11-d680-c993ec198db7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fine tuning a pretrained model:\n",
    "# ...\n",
    "# resnet_ft, accs_ft = train(...)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_rd4a3e_msy"
   },
   "source": [
    "**Learning curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "deletable": false,
    "id": "m0LYF_qq-irH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "401c030f820f72603c8845927e135290",
     "grade": false,
     "grade_id": "cell-dcd99196be31043b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "46cb7f8f-a047-493a-effd-f15b4e0d5ac6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Plot the data with improved style\n",
    "epochs = np.arange(max_epochs)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# Assuming accs_scratch and accs_ft are dictionaries with 'val' as a key\n",
    "# you can use the following lines\n",
    "#ax.plot(epochs, 100 * np.array(accs_scratch['val']), color='skyblue',\n",
    "#        label='Training from Scratch', marker='o', linestyle='-', linewidth=2)\n",
    "#ax.plot(epochs, 100 * np.array(accs_ft['val']), color='lightgreen',\n",
    "#        label='Fine Tuning', marker='s', linestyle='-', linewidth=2)\n",
    "\n",
    "# Set title and axis labels\n",
    "ax.set_title('Validation Accuracy Over Epochs', fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy (%)\", fontsize=14)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=14)\n",
    "\n",
    "# Set grid for better readability\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "# Add legend with a border and shadow\n",
    "ax.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTi5d61ABcA4"
   },
   "source": [
    "The fine-tuning approach has many variations that fit into the broader framework of **transfer learning**. Partial fine-tuning, as illustrated in the following exercise, is one of these variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8V_Cq0D_BRRg"
   },
   "source": [
    "**Exercise:** Instead of retraining all the weights, you can simply use the weights of the classifier. This is referred to as *freezing* the other weights during retraining.\n",
    "- Implement this approach and compare it with the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "EN4cwBcOLQsS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e8e04313c448bae05857d890d16b4c1",
     "grade": false,
     "grade_id": "cell-d8658e2534c37bdf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "12df5892-ff62-479c-cf41-51557dfa16f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# freeze all the layers except the classifier (the last dense layers at end)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "freezed_resnet = resnet.to(device)\n",
    "# train the model\n",
    "# ...\n",
    "# freezed_resnet, accs_freezing = train(...)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "deletable": false,
    "id": "4kvLEMvxs3iS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "42c3c5f160b57ed47e099047a63dd758",
     "grade": false,
     "grade_id": "cell-7528e2b13b4c6001",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "7c0fcbe2-07b5-4bbd-daa9-e7ade6b13733",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# Plot the data with improved style\n",
    "epochs = np.arange(max_epochs)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Set title and axis labels\n",
    "ax.set_title('Validation Accuracy Over Epochs', fontsize=16)\n",
    "ax.set_ylabel(\"Accuracy (%)\", fontsize=14)\n",
    "ax.set_xlabel(\"Epochs\", fontsize=14)\n",
    "\n",
    "# Set grid for better readability\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "# Add legend with a border and shadow\n",
    "ax.legend(frameon=True, fancybox=True, shadow=True, framealpha=0.9, fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaXUV1Ea96gC"
   },
   "source": [
    "In the end, for this small dataset, retraining the last layer performs just as well as global training. To conclude, let's make some predictions with the model on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yxcz-S8rJKD"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=10):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(25,num_images//5*5))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//5, 5, images_so_far)\n",
    "                ax.axis('off')\n",
    "                imshow(inputs.cpu().data[j],ax,'Predicted: {}'.format(class_names[preds[j]]))\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "gzc5gNmAMGCK",
    "outputId": "322986d9-d25c-476f-f75e-ee7c6552b59a"
   },
   "outputs": [],
   "source": [
    "visualize_model(freezed_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h-nTP1jDxAh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
