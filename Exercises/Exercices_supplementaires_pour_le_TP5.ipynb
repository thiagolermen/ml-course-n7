{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Rappels pour le TP n°5\n"
      ],
      "metadata": {
        "id": "AvaKH1dZvg2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 1** : notions de densité, lois marginale, loi jointe, loi conditionnelle, couplage.\n",
        "\n",
        "Soit $X = (X_1, X_2)$ un [vecteur gaussien](https://) de loi $\\mathcal{N}(0, \\Gamma)$ avec $\\Gamma = \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & 1 \\end{pmatrix}$ et $\\epsilon \\in [0,1[$\n",
        "\n",
        "**a.** Quelle est la densité de $X$ ?\n",
        "\n",
        "**b.** Quelles sont les lois marginales de $X$ ?\n",
        "\n",
        "**c.** Calculer la loi conditionnelle de $X_1$ sachant $X_2 = x_2$. Vers quel couplage tend-on lorsque $\\epsilon \\to 1$ ?\n"
      ],
      "metadata": {
        "id": "0Lp3zZE_HAzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** La *Pinball loss*\n",
        "\n",
        "Montrer que la fonction de coût définie par:\n",
        "$$\\mathcal{L}(Y,f_\\theta(x)) = \\rho_t(Y - f_\\theta(x))$$\n",
        "où:\n",
        "$$\\rho_t(u) = t \\times max(u,0) + (t-1) \\times min(u,0) $$\n",
        "\n",
        "\n",
        "est d'espérance minimale lorsque $f_\\theta(x)$ vaut le quantile d'ordre $t$ associé à $p(Y|X=x)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "sX_W4VmBvfZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 3** La divergence de Kullback-Leibler\n",
        "\n",
        "Soient deux lois de probabilités $\\mathcal{P}$ et $\\mathcal{Q}$ admettant comme densités $p$ et $q$, **strictement positives** sur $\\mathbb{R}^d$. La divergence de Kullback-Leibler est donnée par :\n",
        "$$ D_{KL} ( \\mathcal{P} || \\mathcal{Q})=  \\int_{\\mathbb{R^d}} p(x) \\; ln{\\bigg (}\\dfrac{p(x)}{q(x)} {\\bigg)} dx $$\n",
        "\n",
        "**a.** Pourquoi cette quantité est-elle positive? Quand est-elle nulle ?\n",
        "\n",
        "\n",
        "**b.** Calculer cette quantité pour deux lois normales de même variance ($\\sigma^2$) et d'espérances $\\mu_1$ et $\\mu_2$.\n",
        "\n",
        "**c.** Cette quantité définit-elle une distance ?\n",
        "\n",
        "**Remarque** : dans la suite nous utiliserons une version symétrisée de $D_{KL}$. Il s'agit de la divergence de Jensen-Shannon :\n",
        "\n",
        "$$D_{JS} ( \\mathcal{P} || \\mathcal{Q} )  = \\dfrac{1}{2} D_{KL}(\\mathcal{P}||\\mathcal{M}) +  \\dfrac{1}{2} D_{KL}(\\mathcal{Q}||\\mathcal{M})$$ où $\\mathcal{M} = \\dfrac{1}{2} (\\mathcal{Q} + \\mathcal{P})$\n",
        "\n"
      ],
      "metadata": {
        "id": "1wVcX3o6-bU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 4** La distance de Wasserstein\n",
        "\n",
        "Pour définir la version la plus simple de la distance de Wasserstein entre deux distributions $\\mathcal{P}$ et $\\mathcal{Q}$ on suppose seulement qu'elles admettent un moment d'ordre 1. Elle est alors définie par :\n",
        "\n",
        "$$ W(\\mathcal{P},\\mathcal{Q}) = \\underset{\\{\\gamma \\in \\Pi(\\mathcal{P},\\mathcal{Q})\\}}{inf} \\mathbb{E}_{(X,Y) \\sim \\gamma} || X - Y || $$\n",
        "\n",
        "Où $\\Pi(\\mathcal{P},\\mathcal{Q})$ désigne l'ensemble de toutes les lois jointes de marginales $\\mathcal{P}$ et $\\mathcal{Q}$ et $||.||$ désigne la norme usuelle de $\\mathbb{R}^d$.\n",
        "\n",
        "**a.** Reprenons la variable $X$ vue à l'exercice 1.\n",
        "Comment calcule-t-on $\\mathbb{E}_{(X_1,X_2)} | X_2 - X_1 |$ ?\n",
        "\n",
        "\n",
        "**b.** La loi de $X$, à $\\epsilon$ fixé, représente l'une des lois jointes de  $\\Pi(\\mathcal{N}(0,1),\\mathcal{N}(0,1))$. Mais que vaut $W(\\mathcal{N}(0,1),\\mathcal{N}(0,1))$ ?\n",
        "\n",
        "\n",
        "**c.** Quelles sont deux propriétés évidentes de $W$ ?\n",
        "Cette quantité est définie positive et symétrique.\n",
        "\n",
        "\n",
        "**d.** Intuitivement, pour quelle raison la distance de Wasserstein satisfait-elle l'inégalité triangulaire ?\n",
        "\n",
        "\n",
        "\n",
        "**Remarque**: De façon générale, la distance de Wasserstein peut difficilement être calculée directement. Pour l'évaluer, on peut se ramener à un problème d'optimisation grâce à la [dualité de Kantorovitch-Rubinstein](https://cedricvillani.org/sites/dev/files/old_images/2012/08/preprint-1.pdf), qui établit un lien avec les fonctions lipschitziennes:\n",
        "\n",
        "$$ W(\\mathcal{P},\\mathcal{Q}) = \\underset{\\ f \\in \\mathcal{F}_{1}}{sup} {\\bigg [} \\mathbb{E}_{X \\sim \\mathcal{P}} [ f(X) ] -  \\mathbb{E}_{ X \\sim \\mathcal{Q}} [ f(X) ]  {\\bigg ]} $$\n",
        "\n",
        "\n",
        "où $\\mathcal{F}_{1}$ représente l'ensemble des fonctions 1-lipschitziennes sur $\\mathbb{R}^d$."
      ],
      "metadata": {
        "id": "werV73Cn-s7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 5**\n",
        "Reprenons les notations du TP 5 partie 2.\n",
        "D'après [l'article qui introduit les GAN](https://arxiv.org/pdf/1406.2661.pdf), quelles sont les raisons d'espérer la convergence de $\\mathcal{L}_{G(Z)}$ vers $\\mathcal{L}_{X}$ ?"
      ],
      "metadata": {
        "id": "sJPA8J7gDVIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarque:** On peut noter une différence entre le code du TP et le problème de minimax: les poids de $G$ sont mis à jour non pour minimiser  $ln(1 - D(G(z)))$, mais pour minimiser $-ln(D(G(z))$. Les deux sont équivalents mais d'après les auteurs, ce choix permet de renforcer les gradients aux premières étapes de l'apprentissage, lorsque $D$ discrimine très facilement les images réelles des sorties de $G$ (i.e. lorsque $D(G(z)) << 1$)."
      ],
      "metadata": {
        "id": "gsmiJNMsUEN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Exercice n°6** Le principe des Wasserstein GAN.\n",
        "Reprenons le contexte de l'exercice 2 de la partie II du TP5. Dans la version où les poids sont seuillés (WGAN), quelle quantité joue le rôle de fonction de coût lors de l'optimisation de $D$ ?"
      ],
      "metadata": {
        "id": "P4q1BwMBEPtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** La minimisation de la distance de Wasserstein entre la distribution générée et la distribution cible est obtenue par un contrôle des gradients associés au discriminateur. Pour quelle raison, intuitivement ?"
      ],
      "metadata": {
        "id": "yUYcPl_1FLQO"
      }
    }
  ]
}