{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2666ea13-a0b5-42ba-ba46-8624ed2cb995",
   "metadata": {
    "id": "2666ea13-a0b5-42ba-ba46-8624ed2cb995"
   },
   "source": [
    "# Practical nÂ°3: Pixel-scale Prediction - Weakly Supervised Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235c577-43a5-4698-9c3e-f92245eeb63a",
   "metadata": {
    "id": "c235c577-43a5-4698-9c3e-f92245eeb63a"
   },
   "source": [
    "Topics Covered:\n",
    "\n",
    "In Part I:\n",
    "- Testing an FCN-ResNet on a semantic segmentation problem with adapted metrics.\n",
    "- Perfectly supervised learning of a U-Net for image denoising.\n",
    "\n",
    "In Part II:\n",
    "- The noise-to-noise scenario.\n",
    "- Neural Eggs Separation (simplified case).\n",
    "\n",
    "Duration: 4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e922d59-300f-4e0d-ad71-3f584bb87089",
   "metadata": {
    "id": "4e922d59-300f-4e0d-ad71-3f584bb87089"
   },
   "source": [
    "## Part I: Semantic Segmentation with FCN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322901ab-9073-4620-be90-f264fa14ec9c",
   "metadata": {
    "id": "322901ab-9073-4620-be90-f264fa14ec9c"
   },
   "source": [
    "This part aims to familiarize you with a semantic segmentation task. We will use a Fully Convolutional Network (FCN), which was state-of-the-art in 2015 according to [paperswithcode](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k).\n",
    "\n",
    "By definition, a Fully Convolutional Network (FCN) does not contain fully connected layers. As a result, the output retains spatial dimensions. This configuration is useful when the learning target itself is an image. This is the case for tasks such as:\n",
    "- Semantic segmentation, where each pixel is assigned a semantic class (e.g., ground, sky, clouds, buildings, etc.).\n",
    "- Pixel-wise regression\n",
    "- Image denoising\n",
    "- Super-resolution\n",
    "\n",
    "The first exercise features an FCN built from a ResNet50 for a simple segmentation task defined from a set of real images segmented by hand.\n",
    "\n",
    "The second exercise proposes a pixel-wise regression task, completely supervised, defined on a set of dynamically generated synthetic images.\n",
    "\n",
    "**Exercise 1: Semantic Segmentation with FCN-ResNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a950ac-4a53-419e-ba51-32c2961ed8b5",
   "metadata": {
    "id": "b7a950ac-4a53-419e-ba51-32c2961ed8b5"
   },
   "source": [
    "**A.** Presentation of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c946232-77c7-48a6-97a3-b859dc1d4e98",
   "metadata": {
    "id": "3c946232-77c7-48a6-97a3-b859dc1d4e98"
   },
   "source": [
    "In the following cells, we load the necessary libraries, download the set of segmented images prepared for the [Pascal VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/segexamples/index.html) challenge, and visualize input-target pairs from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a87a8a-065c-4d96-9056-96fe3f2f6d89",
   "metadata": {
    "id": "d9a87a8a-065c-4d96-9056-96fe3f2f6d89"
   },
   "outputs": [],
   "source": [
    "#imports de base et montage du drive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.optim as optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2f280-3609-4fc6-9d47-ba9eb29236e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cba2f280-3609-4fc6-9d47-ba9eb29236e7",
    "outputId": "18916826-4cbd-48ec-ab9b-d48ec50ef413"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  print(\"You are on GPU !\")\n",
    "else:\n",
    "  print('Change the runtime to GPU or continue with CPU, but this should slow down your trainings')\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6b382-a270-4e4e-b7e5-95943b600159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bb6b382-a270-4e4e-b7e5-95943b600159",
    "outputId": "b9dbae32-d5eb-45ee-8a7d-8169a120dd09"
   },
   "outputs": [],
   "source": [
    "# dataset pour visualiser (pas de normalisation ni de mise au format torch.tensor)\n",
    "\n",
    "input_resize = transforms.Resize((128, 128))\n",
    "target_resize = transforms.Resize((128, 128))\n",
    "\n",
    "train_dataset_viz = datasets.VOCSegmentation(\n",
    "    './data',\n",
    "    year='2007',\n",
    "    image_set='train',\n",
    "    download = True,\n",
    "    transform=input_resize,\n",
    "    target_transform=target_resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f19ad-c3db-4b4a-a3f4-d7b90941b0bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "bd8f19ad-c3db-4b4a-a3f4-d7b90941b0bc",
    "outputId": "76c7438e-71c9-4eed-e4a4-10323510d819"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_images(images, num_per_row=4, title=None):\n",
    "    num_rows = int(math.ceil(len(images) / num_per_row))\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_per_row,figsize=(4*num_per_row,4*num_rows))\n",
    "    #fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    for image, ax in zip(images, axes.flat):\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Echantilonnage du dataset de viz:\n",
    "\n",
    "inputs, ground_truths = list(zip(*[train_dataset_viz[i] for i in range(8)]))\n",
    "\n",
    "_ = plot_images(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e9396-8ed9-4729-9142-4fd1a6241366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "e74e9396-8ed9-4729-9142-4fd1a6241366",
    "outputId": "3fffac25-58ca-42bb-c485-0f2b51a55ff3"
   },
   "outputs": [],
   "source": [
    "_ = plot_images(ground_truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478d313-a90d-420a-a74a-2e6c78b5ae44",
   "metadata": {
    "id": "0478d313-a90d-420a-a74a-2e6c78b5ae44"
   },
   "source": [
    "**Question 1:** How many classes are there? \\\\\n",
    "Search the web for the difference between semantic segmentation and instance segmentation. What type of segmentation is this dataset about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TSQMGJaS5Pqs",
   "metadata": {
    "deletable": false,
    "id": "TSQMGJaS5Pqs",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84775feb7f536cab2b49a4dcef1b48e6",
     "grade": false,
     "grade_id": "cell-9636b6792aadc531",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_classes = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MC1WQOva5s8j",
   "metadata": {
    "deletable": false,
    "id": "MC1WQOva5s8j",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92cdba4502e11e327c6a7ea6d54c7de5",
     "grade": true,
     "grade_id": "cell-b2ebe011b0b5e6f6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad29c10-d73a-4e9e-a68f-3de640091792",
   "metadata": {
    "id": "2ad29c10-d73a-4e9e-a68f-3de640091792"
   },
   "source": [
    "**B.** Presentation of an FCN-ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67298d17-176b-4f62-af9b-083aee1b52c3",
   "metadata": {
    "id": "67298d17-176b-4f62-af9b-083aee1b52c3"
   },
   "source": [
    "In the next cell, we load an [FCN](https://pytorch.org/vision/stable/models/fcn.html) built from a [ResNet50](https://arxiv.org/pdf/1512.03385.pdf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30500a47-f15e-420a-83eb-1f977a4fbedd",
   "metadata": {
    "id": "30500a47-f15e-420a-83eb-1f977a4fbedd"
   },
   "outputs": [],
   "source": [
    "fcn = torchvision.models.segmentation.fcn_resnet50(weights_backbone = None )\n",
    "# fcn = torchvision.models.segmentation.fcn_resnet50( weights_backbone = torchvision.models.ResNet50_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13c65d5-8231-4071-99ec-a1295f2bd924",
   "metadata": {
    "id": "c13c65d5-8231-4071-99ec-a1295f2bd924"
   },
   "source": [
    "**Q2:** What is different from a standard ResNet50? Does the FCN provide an output of the same size as the input? Test and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c3b5f-18c2-4cce-a922-eedf42bdb61c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd8c3b5f-18c2-4cce-a922-eedf42bdb61c",
    "outputId": "fa098d0e-0bd8-4da7-87ae-c22b2dc97095"
   },
   "outputs": [],
   "source": [
    "resnet50 =  torchvision.models.resnet50()\n",
    "print(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7e6db-f35b-4934-801c-3260035a9c19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4af7e6db-f35b-4934-801c-3260035a9c19",
    "outputId": "5b569fcb-c3ec-44f3-b2b3-4870510786e8"
   },
   "outputs": [],
   "source": [
    "print(fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c673afd6-c837-44af-b066-8733f2cb123b",
   "metadata": {
    "id": "c673afd6-c837-44af-b066-8733f2cb123b"
   },
   "source": [
    "In summary:\n",
    "- The final perceptron is replaced by a fully convolutional \"head\" ([FCNhead](https://github.com/pytorch/vision/blob/main/torchvision/models/segmentation/fcn.py)).\n",
    "- In the case of ResNet, the stride = 2 is replaced by dilation.\n",
    "- The forward pass involves an interpolation step at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71429bb-2af0-4687-853c-d5f87ea366a5",
   "metadata": {
    "id": "c71429bb-2af0-4687-853c-d5f87ea366a5"
   },
   "outputs": [],
   "source": [
    "# sample the dataset, convert to torch.tensor:\n",
    "batch_size = 4\n",
    "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "input_resize = transforms.Resize([64,64])\n",
    "target_resize = transforms.Resize((64,64))\n",
    "\n",
    "input_transform = transforms.Compose(\n",
    "    [\n",
    "        input_resize,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b61743-fde9-4706-8beb-7cfc7e816830",
   "metadata": {
    "id": "b2b61743-fde9-4706-8beb-7cfc7e816830"
   },
   "outputs": [],
   "source": [
    "def replace_tensor_value_(tensor, a, b):\n",
    "    tensor[tensor == a] = b\n",
    "    return tensor\n",
    "\n",
    "target_transform = transforms.Compose(\n",
    "    [\n",
    "        target_resize,\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.Lambda(lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 21)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b7bf0-4019-4473-b173-4a1f1ce0d6e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e5b7bf0-4019-4473-b173-4a1f1ce0d6e9",
    "outputId": "72ea7c20-42c2-4130-fa4b-ebec30f8a0c9"
   },
   "outputs": [],
   "source": [
    "# Creation d'un dataset\n",
    "train_dataset = datasets.VOCSegmentation(\n",
    "    './data',\n",
    "    year='2007',\n",
    "    download=False,\n",
    "    image_set='train',\n",
    "    transform=input_transform,\n",
    "    target_transform=target_transform,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "inputs, targets  = next(iter(train_loader))\n",
    "\n",
    "inputs = inputs\n",
    "y = fcn(inputs)\n",
    "print(y['out'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bae8c-016d-4486-9b15-ba39c1d7beea",
   "metadata": {
    "id": "514bae8c-016d-4486-9b15-ba39c1d7beea"
   },
   "source": [
    "The image size is mechanically preserved by interpolation. While a stride = 2 implies a spatial dimension reduction by half, this is not the case with a dilation = 2 / 4 (see this [illustration](https://github.com/vdumoulin/conv_arithmetic)). Thus, the spatial dimensions of the feature maps are less reduced than with the original ResNet50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354b3a6-fecb-4fe3-8b9e-ad3ff5f62b67",
   "metadata": {
    "id": "f354b3a6-fecb-4fe3-8b9e-ad3ff5f62b67"
   },
   "source": [
    "**C. Testing a Pretrained Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5be37f2-bf35-4060-97a8-0f3d8c0c698a",
   "metadata": {
    "id": "c5be37f2-bf35-4060-97a8-0f3d8c0c698a"
   },
   "source": [
    "In this exercise, we simply **test** a model trained on another segmentation dataset. An extension to this exercise provides an opportunity to train the model introduced in exercise sheet #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4a0f3-108f-47f5-b7fe-7452547a4c27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ad4a0f3-108f-47f5-b7fe-7452547a4c27",
    "outputId": "04fc836a-24c2-4cef-88ae-1f8a9d9afb07"
   },
   "outputs": [],
   "source": [
    "fcn = torchvision.models.segmentation.fcn_resnet50(weights='COCO_WITH_VOC_LABELS_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab4d24-23d5-487c-9dbe-e5625fbfc44d",
   "metadata": {
    "id": "31ab4d24-23d5-487c-9dbe-e5625fbfc44d"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "input_resize = transforms.Resize((256,256))\n",
    "target_resize = transforms.Resize((256,256))\n",
    "\n",
    "input_transform = transforms.Compose(\n",
    "    [\n",
    "        input_resize,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = transforms.Compose(\n",
    "    [\n",
    "        target_resize,\n",
    "        transforms.PILToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = datasets.VOCSegmentation(\n",
    "    './data',\n",
    "    year='2007',\n",
    "    download=False,\n",
    "    image_set='val',\n",
    "    transform=input_transform,\n",
    "    target_transform=target_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c498bcc-fcfc-44f2-a30f-2a0747e04075",
   "metadata": {
    "id": "6c498bcc-fcfc-44f2-a30f-2a0747e04075"
   },
   "outputs": [],
   "source": [
    "# Creating loaders\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6l5HcC7VAq4C",
   "metadata": {
    "deletable": false,
    "id": "6l5HcC7VAq4C",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0626268630ea9395b057942927f71e7",
     "grade": true,
     "grade_id": "cell-840366ac0521f360",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**Q3:** According to the preceding code lines, which set (validation or test) of PascalVOC2007 are we testing the model on? Why?\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c815484-4636-4659-90dc-52af6f299b69",
   "metadata": {
    "id": "2c815484-4636-4659-90dc-52af6f299b69"
   },
   "source": [
    "Now, let's visualize some model outputs on this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e20b2e-6317-4c3c-b099-cf4fef58b5fe",
   "metadata": {
    "id": "73e20b2e-6317-4c3c-b099-cf4fef58b5fe"
   },
   "outputs": [],
   "source": [
    "# Color palette for segmentation masks\n",
    "PALETTE = np.array(\n",
    "    [\n",
    "        [0, 0, 0],\n",
    "        [128, 0, 0],\n",
    "        [0, 128, 0],\n",
    "        [128, 128, 0],\n",
    "        [0, 0, 128],\n",
    "        [128, 0, 128],\n",
    "        [0, 128, 128],\n",
    "        [128, 128, 128],\n",
    "        [64, 0, 0],\n",
    "        [192, 0, 0],\n",
    "        [64, 128, 0],\n",
    "        [192, 128, 0],\n",
    "        [64, 0, 128],\n",
    "        [192, 0, 128],\n",
    "        [64, 128, 128],\n",
    "        [192, 128, 128],\n",
    "        [0, 64, 0],\n",
    "        [128, 64, 0],\n",
    "        [0, 192, 0],\n",
    "        [128, 192, 0],\n",
    "        [0, 64, 128],\n",
    "    ]\n",
    "    + [[0, 0, 0] for i in range(256 - 22)]\n",
    "    + [[255, 255, 255]],\n",
    "    dtype=np.uint8,\n",
    ")\n",
    "\n",
    "\n",
    "def array1d_to_pil_image(array):\n",
    "    pil_out = Image.fromarray(array.astype(np.uint8), mode='P')\n",
    "    pil_out.putpalette(PALETTE)\n",
    "    return pil_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfa5b3-75c9-4af1-9b7c-f0cfbd02355d",
   "metadata": {
    "id": "d0bfa5b3-75c9-4af1-9b7c-f0cfbd02355d"
   },
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(test_loader))\n",
    "outputs = fcn(inputs)['out']\n",
    "outputs = outputs.argmax(1)\n",
    "\n",
    "outputs = replace_tensor_value_(outputs, 21, 255)\n",
    "targets = replace_tensor_value_(targets, 21, 255)\n",
    "targets = targets.squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e48ab-584f-4c58-8284-881c29b6fa37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9c7e48ab-584f-4c58-8284-881c29b6fa37",
    "outputId": "9b043299-ab03-4821-d474-b1c06728fd70"
   },
   "outputs": [],
   "source": [
    "plt_inputs = np.clip(inputs.numpy().transpose((0, 2, 3, 1)) * imagenet_std + imagenet_mean, 0, 1)\n",
    "fig = plot_images(plt_inputs)\n",
    "fig.suptitle(\"Images\")\n",
    "\n",
    "pil_outputs = [array1d_to_pil_image(out) for out in outputs.numpy()]\n",
    "fig = plot_images(pil_outputs)\n",
    "fig.suptitle(\"Predictions\")\n",
    "\n",
    "pil_targets = [array1d_to_pil_image(gt) for gt in targets.numpy()]\n",
    "fig = plot_images(pil_targets)\n",
    "_ = fig.suptitle(\"Ground truths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffe212-798a-4ee7-9f9f-f3ce99283f66",
   "metadata": {
    "id": "d8ffe212-798a-4ee7-9f9f-f3ce99283f66"
   },
   "source": [
    "Now, let's evaluate the model on the entire set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92deea8-818e-4dfb-bb61-ce952f86188b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f92deea8-818e-4dfb-bb61-ce952f86188b",
    "outputId": "59b4f024-4653-4984-dbbe-11b10d29ce50"
   },
   "outputs": [],
   "source": [
    "# For the test metric\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc4832-79eb-403e-a665-ecb8371b75e2",
   "metadata": {
    "id": "c1cc4832-79eb-403e-a665-ecb8371b75e2"
   },
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "IoU = torchmetrics.JaccardIndex(num_classes=21, ignore_index=255,task=\"multiclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffe4f6-c159-4e3d-9200-47c2afa85d9f",
   "metadata": {
    "deletable": false,
    "id": "4bffe4f6-c159-4e3d-9200-47c2afa85d9f",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32bb09bc26d4b4f80db77b4b75a63777",
     "grade": true,
     "grade_id": "cell-d0d00b1251df1c27",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Q4: Jaccard Index is used instead of accuracy. How is it defined? What is its other name? What is its advantage?\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfd8be-aeda-4d7a-8f3a-7c97103f4caf",
   "metadata": {
    "id": "1ccfd8be-aeda-4d7a-8f3a-7c97103f4caf"
   },
   "source": [
    "Q5: Modify the following code to obtain an average IoU over the entire set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e346cbb-87b2-4481-9856-f12cd989f20f",
   "metadata": {
    "id": "0e346cbb-87b2-4481-9856-f12cd989f20f"
   },
   "outputs": [],
   "source": [
    "fcn = fcn.to(device)\n",
    "fcn.eval()\n",
    "for i, (inputs, targets) in enumerate(test_loader):\n",
    "  targets = targets.squeeze(dim=1)\n",
    "  targets[targets==255]= 0\n",
    "  inputs = inputs.to(device)\n",
    "  targets = targets.to(device)\n",
    "  outputs = fcn(inputs)['out']\n",
    "\n",
    "  outputs = outputs.cpu()\n",
    "  targets = targets.cpu()\n",
    "  batch_IoU = IoU(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cxyeO2UYIVy6",
   "metadata": {
    "id": "cxyeO2UYIVy6"
   },
   "source": [
    "**Exercise 2: Perfectly Supervised Denoising**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MyxF-5a-ITtd",
   "metadata": {
    "id": "MyxF-5a-ITtd"
   },
   "source": [
    "In a perfectly supervised denoising problem, we have two sets of images $B_{noisy}$ and $B_{clean}$.\n",
    "Each clean image in $B_{clean}$ corresponds to a noisy version in $B_{noisy}$. \\\\\n",
    "In this exercise, we will train an FCN to transform the noisy version into the clean one. To avoid using real data that would take up space on the drive, we work on synthetic images generated on the fly. We are thus in the ideal scenario where the datasets are infinitely large, and overfitting is not a concern.\n",
    "In this context, data augmentation and validation steps are unnecessary, simplifying the training procedures.\n",
    "The following cells visualize data from $B_{noisy}$ and their corresponding \"clean\" counterparts. The function $gen$ (in the module  **utile.py**) samples batches on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81b943d-4b2f-4314-9f4a-d53631db4cce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c81b943d-4b2f-4314-9f4a-d53631db4cce",
    "outputId": "95d34ac8-80b1-469e-fff2-174e2216a88a"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/relmonta/ml-student.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c719f-acc0-400d-a30a-deb84e2a6085",
   "metadata": {
    "id": "560c719f-acc0-400d-a30a-deb84e2a6085"
   },
   "outputs": [],
   "source": [
    "! cp ml-student/TP3/utile.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7367e-e140-4fb2-ad5a-97904a6a56dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "5bb7367e-e140-4fb2-ad5a-97904a6a56dd",
    "outputId": "edd57cce-aef2-44de-9eb8-6592c9fd009b"
   },
   "outputs": [],
   "source": [
    "from utile import *\n",
    "\n",
    "input, target = gen(6)\n",
    "\n",
    "# Noisy versions (filled and noisy rectangles)\n",
    "fig0 = plt.figure(0, figsize=(36, 6))\n",
    "voir_batch2D(input, 6, fig0, k=0, min_scale=0, max_scale=1)\n",
    "\n",
    "# Clean versions (only cells)\n",
    "fig1 = plt.figure(1, figsize=(36, 6))\n",
    "voir_batch2D(target, 6, fig1, k=0, min_scale=0, max_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62c467-2edd-4fae-b73b-a75dc66a2d47",
   "metadata": {
    "id": "0d62c467-2edd-4fae-b73b-a75dc66a2d47"
   },
   "source": [
    "The task is to remove the rectangles (filled or not) from the image. It is a well-posed but a priori challenging problem: spatial context is necessary to achieve it. To address this, we will use another popular FCN: a simplified [U-Net](https://arxiv.org/abs/1505.04597) (see utile.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063992f-8cb0-4786-ad30-11766a9ce0f3",
   "metadata": {
    "id": "d063992f-8cb0-4786-ad30-11766a9ce0f3"
   },
   "outputs": [],
   "source": [
    "ch_in = 1\n",
    "ch_out = 1\n",
    "size = 16\n",
    "\n",
    "fcn = UNet(ch_in, ch_out, size).to(device)  # 1 input channel, 1 output channel, network size parameter: 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d163b-0fdd-49f1-ae76-6e889815aceb",
   "metadata": {
    "id": "b60d163b-0fdd-49f1-ae76-6e889815aceb"
   },
   "source": [
    "**Q1:** How many convolution layers does this U-Net contain? How many weights in total when $size = 16$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74504800-b929-4f8f-9f8b-6f19d45110d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74504800-b929-4f8f-9f8b-6f19d45110d7",
    "outputId": "ef38ddd9-3617-42c0-f472-59ae42ab58ae"
   },
   "outputs": [],
   "source": [
    "#print(fcn)\n",
    "i=0\n",
    "for module in fcn.modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "      i+=1\n",
    "\n",
    "print(i)\n",
    "\n",
    "pconv=0\n",
    "pconvt=0\n",
    "pbn=0\n",
    "ptot=0\n",
    "\n",
    "for module in fcn.modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "      for parameter in module.parameters():\n",
    "        pconv += torch.numel(parameter) #numel: counts the number of elements in a matrix\n",
    "\n",
    "    if isinstance(module, nn.ConvTranspose2d):\n",
    "      for parameter in module.parameters():\n",
    "        pconvt += torch.numel(parameter) #numel: counts the number of elements in a matrix\n",
    "\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "      for parameter in module.parameters():\n",
    "        pbn += torch.numel(parameter)\n",
    "\n",
    "print(pconv)  # number of weights in convolution layers\n",
    "print(pconvt) #  \" \" in transpose convolution layers\n",
    "print(pbn)    # number of weights in BatchNorm layers\n",
    "\n",
    "print(pconv + pconvt + pbn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283f11bf-270a-46e3-9072-697c4e2a45f3",
   "metadata": {
    "id": "283f11bf-270a-46e3-9072-697c4e2a45f3"
   },
   "source": [
    "**Q2:** For an input image with spatial dimensions $64\\times 64$, what is the dimension of the intermediate feature map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5133a9-e427-41c3-a95e-166e07f858bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae5133a9-e427-41c3-a95e-166e07f858bc",
    "outputId": "5e609d7b-d079-489c-c209-ab0ddfaabf90"
   },
   "outputs": [],
   "source": [
    "fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2a441-eb46-41a0-a2de-5d67cd17cc23",
   "metadata": {
    "id": "e9b2a441-eb46-41a0-a2de-5d67cd17cc23"
   },
   "source": [
    "The successive convolutions preserve the dimensions (due to \"padding\" - a layer of zeros on each side). The four successive maxpooling operations reduce the spatial dimensions to $4 \\times 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430736ae-866d-48d5-bc3f-bbe05c5ccde2",
   "metadata": {
    "id": "430736ae-866d-48d5-bc3f-bbe05c5ccde2"
   },
   "source": [
    "**Q3:** How is the multi-scale processing ensured? What is the purpose of \"skip-connections\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffef15-1f8d-443a-bffd-2653662a53a9",
   "metadata": {
    "deletable": false,
    "id": "49ffef15-1f8d-443a-bffd-2653662a53a9",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8e2450c5a68f75f6a2a137589e0a6c2",
     "grade": true,
     "grade_id": "cell-a6df0366b6e381d7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01a417-5cf3-4397-8170-67b329898019",
   "metadata": {
    "id": "cb01a417-5cf3-4397-8170-67b329898019"
   },
   "source": [
    "**Q4:** Define a simple training procedure (no validation) using Mean Square Error (MSE) as the cost function and Adam as the optimization method (learning rate of 0.001). Once the training procedure is ready, associate a GPU with the notebook and ensure that the denoising task is well learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca7773-6945-41e9-bc4f-d853386f7c46",
   "metadata": {
    "id": "59ca7773-6945-41e9-bc4f-d853386f7c46"
   },
   "outputs": [],
   "source": [
    "fcn = fcn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08555c72-b5db-4b3a-966d-a7a5e51ea08a",
   "metadata": {
    "id": "08555c72-b5db-4b3a-966d-a7a5e51ea08a"
   },
   "outputs": [],
   "source": [
    "def criterion(output, target):\n",
    "    return torch.mean((output - target)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45b419-f1f6-4dad-bb09-661d7ab0a451",
   "metadata": {
    "id": "0c45b419-f1f6-4dad-bb09-661d7ab0a451"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(fcn.parameters(), 10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47f885-862f-4205-a316-6045aea86aa8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "cc47f885-862f-4205-a316-6045aea86aa8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b30350cdfd80acd397e30becd0957306",
     "grade": false,
     "grade_id": "cell-631d663900b2f330",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "12603083-cb1d-47a3-cfe9-52683a7cffd1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nepochs = 40  # brew a tea while waiting\n",
    "nbatches = 100  # Number of batches per epoch\n",
    "batchsize = 32  # Batch size\n",
    "\n",
    "train_losses = []  # List to store training losses\n",
    "\n",
    "fcn = fcn.to(device)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    print(f\"Epoch {epoch + 1}/{nepochs}\")\n",
    "\n",
    "    epoch_losses = []  # List to store losses for each batch in the epoch\n",
    "\n",
    "    for i in range(nbatches):\n",
    "        # Load inputs\n",
    "        inputs, targets = gen(batchsize)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b82258-13cc-4015-bbc8-33d59eccb9bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "81b82258-13cc-4015-bbc8-33d59eccb9bb",
    "outputId": "e5f2568c-b875-4c7a-f5db-ff410dd3dde6"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ef0ee-0cf3-402b-a41e-631b1836d9e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 789
    },
    "deletable": false,
    "id": "de7ef0ee-0cf3-402b-a41e-631b1836d9e4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e41913fe0b74b6762f894323e81e3955",
     "grade": false,
     "grade_id": "cell-1b71282773a481aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "07a60a75-0d8c-41f2-a277-4d66ee165e22"
   },
   "outputs": [],
   "source": [
    "#visualization:\n",
    "\n",
    "fcn.eval()\n",
    "\n",
    "target = make_batch(6, rec=0., noisy_rec=0., disc=0.002).to(device)\n",
    "noise = make_batch(6, rec=0.0003, noisy_rec=0.0003, disc=0.).to(device)\n",
    "input = target + noise\n",
    "\n",
    "# output =\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
