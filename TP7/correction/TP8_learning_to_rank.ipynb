{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSgiKnAs1BbN"
   },
   "source": [
    "# **Practical session nÂ°8** : Learning-to-Rank and Re-identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1waEt7NJgSx"
   },
   "source": [
    "**Part I:**\n",
    "\n",
    "Several problems fall under the label \"[learning-to-rank](https://link.springer.com/content/pdf/10.1007/978-3-642-15880-3_20.pdf).\" One example is sorting a given list of labels (e.g. URL of websites) by relevance to an input query. This problem is referred to as *label ranking*.\\\n",
    " In another scenario, input objects (e.g. images) should be sorted wrt a given criterion (e.g. 'foggy' or 'snowy'). Terms like *object ranking* and *learning to order things* are often used to describe this situation. In both cases, the learning is based on sorted samples, such as pairs of ordered images.\n",
    "\n",
    "In this practical session, we illustrate the latter scenario using very simple synthetic images. All images consist of a mixture of a disc and a variable number of rectangles of different shapes. The goal is to sort the images based on the pixel intensity on the disc. To achieve this, we work in a standard context where we have pairs of ordered images. Using these pairs, we will train a neural network to construct a real-valued \"ranking function\" (*ranker*) whose outputs enable the sorting of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtq1hUDdoSDC"
   },
   "source": [
    "**Exercise #1:** Problem Construction\n",
    "\n",
    "The following cells enable you to:\n",
    "- generate a dataset on your drive (train+val and test),\n",
    "- define a dataset that provides pairs of images and a comparison based on the criterion of disc intensity (\"0\" if the disc is more intense in the first image, \"1\" otherwise),\n",
    "- visualize an initial batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5yBBrlb5gia"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "ls = lambda rep: sorted(os.listdir(rep))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from random import randint, choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88-qgWv1k5im"
   },
   "outputs": [],
   "source": [
    "# imports from a git repo\n",
    "git clone https://github.com/relmonta/ml-student.git\n",
    "os.chdir('ml-student/TP8')\n",
    "\n",
    "from archis import *\n",
    "from utile_tp8_partI import *\n",
    "#from train_and_test import *\n",
    "\n",
    "root = r\"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ubPaW6jE3n-"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DM4QiDC95gak"
   },
   "outputs": [],
   "source": [
    "dir_trainval = join(root, r\"train\")\n",
    "generate_dataset(dir_trainval, size_dataset=10000)\n",
    "\n",
    "dir_test = join(root, r\"test\")\n",
    "generate_dataset(dir_test, size_dataset=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v04NNmhgBbY6"
   },
   "outputs": [],
   "source": [
    "# Retrieving target values\n",
    "\n",
    "# Image paths:\n",
    "dir_images_trainval = os.path.join(dir_trainval, 'images')\n",
    "dir_images_test = os.path.join(dir_test, 'images')\n",
    "\n",
    "# Target values for train+val\n",
    "label_dict_path_trainval = os.path.join(dir_trainval, 'labels_synthese.pickle')\n",
    "with open(label_dict_path_trainval, 'rb') as handle:\n",
    "    label_dict_trainval = pickle.load(handle)\n",
    "\n",
    "# Target values for test\n",
    "label_dict_path_test = os.path.join(dir_test, 'labels_synthese.pickle')\n",
    "with open(label_dict_path_test, 'rb') as handle:\n",
    "    label_dict_test = pickle.load(handle)\n",
    "\n",
    "# Splitting train / val (8000/2000)\n",
    "all_image_names = np.array(ls(dir_images_trainval))\n",
    "\n",
    "train_indices = list(range(0, 8000))\n",
    "names_train = all_image_names[train_indices]\n",
    "val_indices = list(range(8000, 10000))\n",
    "names_val = all_image_names[val_indices]\n",
    "names_test = ls(dir_images_test)\n",
    "\n",
    "# Note: for random splitting, use sklearn.model_selection.train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pC2CQnsDREe"
   },
   "outputs": [],
   "source": [
    "# si erreur :\n",
    "# from shutil import rmtree\n",
    "# rmtree(dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv4HHS6Y67lE"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "class SuperFlip(object):\n",
    "    \"\"\"\n",
    "    The 8 transformations\n",
    "    generated by R(Pi/2) and vertical symmetry/axis\n",
    "    \"\"\"\n",
    "    def __init__(self, num_transforms):\n",
    "        self.num_transforms = num_transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Note: Ideally, torch.randint should be used here...\n",
    "        n = randint(0, self.num_transforms)\n",
    "        if n == 1:\n",
    "            image = image.flip([1])\n",
    "        elif n == 2:\n",
    "            image = image.flip([2])\n",
    "        elif n == 3:\n",
    "            image = image.transpose(1, 2)\n",
    "        elif n == 4:\n",
    "            image = image.transpose(1, 2).flip([1])\n",
    "        elif n == 5:\n",
    "            image = image.transpose(1, 2).flip([2])\n",
    "        elif n == 6:\n",
    "            image = image.flip([1, 2])\n",
    "        elif n == 7:\n",
    "            image = image.transpose(1, 2).flip([1, 2])\n",
    "        return image\n",
    "\n",
    "super_flip_transform = SuperFlip(8)\n",
    "\n",
    "transforms = {\n",
    "    'train': super_flip_transform,\n",
    "    'val': None,\n",
    "    'test': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7Qf6WY36-Fo"
   },
   "outputs": [],
   "source": [
    "# Dataset Construction:\n",
    "def oracle(name0, name1, data_dict):\n",
    "    # Load the data:\n",
    "    y0 = data_dict[name0]['y']\n",
    "    y1 = data_dict[name1]['y']\n",
    "\n",
    "    # Determine the comparison:\n",
    "    compa = 0 if y1 < y0 else 1\n",
    "    return compa\n",
    "\n",
    "class DatasetOrderedPairs(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, data_dict, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.imgs = ls(images_dir)\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name0 = self.imgs[idx]\n",
    "        name1 = choice(self.imgs)\n",
    "        label = oracle(name0, name1, self.data_dict)\n",
    "\n",
    "        # Get the images\n",
    "        path0 = os.path.join(self.images_dir, name0)\n",
    "        img0 = torch.load(path0)\n",
    "        path1 = os.path.join(self.images_dir, name1)\n",
    "        img1 = torch.load(path1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "\n",
    "        return img0, img1, torch.from_numpy(np.array(label)).long(), name0, name1   # -1 if no class 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GI1DH45t7AcN"
   },
   "outputs": [],
   "source": [
    "# Dataset instantiation:\n",
    "dataset_train = DatasetOrderedPairs(dir_images_trainval, label_dict_trainval, transforms['train'])\n",
    "dataset_val = DatasetOrderedPairs(dir_images_trainval, label_dict_trainval, transforms['val'])\n",
    "dataset_test = DatasetOrderedPairs(dir_images_test, label_dict_test, transforms['test'])\n",
    "\n",
    "datasets = {'train': dataset_train, 'val': dataset_val, 'test': dataset_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiM4F3gaAzw3"
   },
   "outputs": [],
   "source": [
    "# Samplers and loaders\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "\n",
    "samplers = {'train': train_sampler, 'val': val_sampler}\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=batch_size, shuffle=False, sampler=samplers[x], num_workers=2) for x in ['train', 'val']}\n",
    "dataloaders['test'] = torch.utils.data.DataLoader(datasets['test'], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "dataset_sizes = {'train': len(names_train), 'val': len(names_val), 'test': len(names_test)}\n",
    "\n",
    "dataloaders['viz'] = torch.utils.data.DataLoader(datasets['train'], batch_size=6, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0Qib3bU7EUO"
   },
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "\n",
    "img1, img2, labels, _, _ = next(iter(dataloaders['viz']))\n",
    "\n",
    "fig0 = plt.figure(0, figsize=(15, 3))\n",
    "voir_batch2D(img1, nx = 8, fig = fig0, k=0, min_scale=0,max_scale=10)\n",
    "fig1 = plt.figure(1, figsize=(15, 3))\n",
    "voir_batch2D(img2, nx = 8, fig = fig1, k=0, min_scale=0,max_scale=10)\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iycy2im3U8wg"
   },
   "source": [
    "**Q0** How is done the separation between training and validation done here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QR_3fcbEBJg"
   },
   "source": [
    "**Q1** What is the role of *super_flip*? And that of the *oracle* function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ln7gZSUGEnmU"
   },
   "source": [
    "**Q2** Are all pairs of images equally easy to order?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFv2OfMMF1mF"
   },
   "source": [
    "**Exercise #2:** Siamese Learning\n",
    "\n",
    "During training, batches of image pairs are compared. Basic siamese network training involves passing each image in the pair independently through the model and penalizing the model when the outputs are arranged in the wrong order.\n",
    "\n",
    "The simplest way to do this is to consider the positive part of the difference between the outputs. This is what the following cost function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYZtrZy07G-r"
   },
   "outputs": [],
   "source": [
    "# Hinge Loss function\n",
    "\n",
    "def label_to_sgn(label):  # 0 -> 1  and 1 -> -1\n",
    "    sgn = torch.where(label == 0, 1, -1)\n",
    "    return sgn\n",
    "\n",
    "class HingeLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=0.1):\n",
    "        super(HingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output0, output1, label):\n",
    "        sgn = label_to_sgn(label)\n",
    "        diff = sgn * (output1 - output0)\n",
    "\n",
    "        loss = torch.relu(diff + self.margin).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmVCCdfxUN86"
   },
   "source": [
    "**Q1** Write the training loop and run it for 20 epochs. Keep track of the successive accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04USzeWb7I8m"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "channels = 1\n",
    "\n",
    "# With a ResNet18\n",
    "from torchvision.models import resnet18\n",
    "model = resnet18(num_classes=1)\n",
    "model.conv1 =  nn.Conv2d(channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# Init the new conv layer\n",
    "nn.init.kaiming_normal_(model.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = HingeLoss(0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.002 )\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = HingeLoss(margin=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1wHjBb77Ly5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, notebook\n",
    "\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "phases = ['train', 'val']\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in phases:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        data_loader = notebook.tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}', leave= phase == 'train')\n",
    "        # Iterate over data.\n",
    "        for img1, img2, labels, _, _ in data_loader:\n",
    "\n",
    "\n",
    "\n",
    "            # TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * img1.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data).double().item()\n",
    "\n",
    "            data_loader.set_postfix(loss=f'{loss.item():.4f}', refresh=False)\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        if phase == 'train':\n",
    "            train_accs.append(epoch_acc)\n",
    "\n",
    "        if phase == 'val':\n",
    "            val_accs.append(epoch_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU951Grd1tWR"
   },
   "source": [
    "**Q3** Check the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPhXKjr7VW0a"
   },
   "source": [
    "**Q4** Interprete the following scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSL2b6Ro7P_l"
   },
   "outputs": [],
   "source": [
    "# true (mean) pixel intensity on the disk:\n",
    "ys = []\n",
    "# outputs of the network:\n",
    "yhats = []\n",
    "\n",
    "# browse the test set:\n",
    "for name in names_test:\n",
    "    # get true mean intensity on the disk\n",
    "    y = label_dict_test[name]['y']\n",
    "    ys.append(y)\n",
    "\n",
    "    path = join(dir_images_test, name)\n",
    "    image = torch.load(path)\n",
    "    image = image.cuda().unsqueeze(dim=0)\n",
    "\n",
    "    # get model output\n",
    "    yhat = model.eval()(image)\n",
    "    yhat = yhat.item()\n",
    "    yhats.append(yhat)\n",
    "\n",
    "# make np array\n",
    "ys = np.array(ys)\n",
    "yhats = np.array(yhats)\n",
    "\n",
    "# scatterplot\n",
    "plt.figure(num=10)\n",
    "plt.scatter(ys, yhats, marker='+')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('$\\hat{y}$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIiUvbAi1WYq"
   },
   "source": [
    "**Q5** Calculate the Spearman and Kendall rank correlations. Which of the two is related to the accuracy measure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvImPV2v7SbT"
   },
   "source": [
    "**Q6** We extend the training to 50 epochs. Load the following checkpoint, revisit questions 4 and 5 with this model, and provide comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFYUjVAohKgn"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "! wget https://www.grosfichiers.com/WUuEwrxaf65_vySRhNAbepk\n",
    "PATH_checkpoint = './WUuEwrxaf65_vySRhNAbepk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZpLh5fFMzmq"
   },
   "source": [
    "**Q7** How would you improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw6CdRxcV0AC"
   },
   "source": [
    "Curriculum Learning:\n",
    "- Seek more \"challenging\" pairs towards the end of training.\n",
    "\n",
    "Optimization:\n",
    "- Consider adding a learning rate scheduler.\n",
    "\n",
    "Other Loss Functions:\n",
    "- Explore RankNet loss.\n",
    "- Explore Listnet Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mokDJcGBncXF"
   },
   "source": [
    "**Exercice #3** Curriculum Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZouHyg51niH8"
   },
   "source": [
    "One idea is to make the problem harder after an initial training phase. The following code allows you to retrieve a ResNet18 trained for 50 epochs, the optimizer, and the associated learning curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cbFjkshyhB4"
   },
   "source": [
    "**Q1** Take the ResNet18 trained on 50 epochs. Continue training for an additional 20 epochs without changing the dataloader. Remember to store accuracies related to the validation set and outputs related to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzxxFnHNF8_X"
   },
   "source": [
    "**Q2** We will now continue training on pairs that are harder to order. For this purpose, we have the 'Dataset_finer_pairs' dataset below. Train for 20 epochs using this dataset (use epoch_ray = 1.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGG_CL2GF8JY"
   },
   "outputs": [],
   "source": [
    "def make_dic_ray(imgs, dic, ray):\n",
    "\n",
    "  ys = np.array([dic[img]['y'] for img in imgs])\n",
    "  imgs = np.array(imgs)\n",
    "  dic_ray = {}\n",
    "  for i,img in enumerate(imgs):\n",
    "    y = ys[i]\n",
    "    # on limite la paire Ã  des disques proches en intensitÃ©\n",
    "    dic_ray[img] = list(imgs[np.abs(ys - y) < ray])\n",
    "\n",
    "  return dic_ray\n",
    "\n",
    "class Dataset_finer_pairs(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir,  dic, transfo = None, ray=0.5):\n",
    "        self.images_dir = images_dir\n",
    "        self.transfo = transfo\n",
    "        self.imgs = sorted(ls(images_dir))\n",
    "        self.dic = dic\n",
    "        self.dic_ray = make_dic_ray(self.imgs, dic, ray)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        name0 = self.imgs[idx]\n",
    "        name1 = choice(self.dic_ray[name0])\n",
    "        label = oracle(name0, name1, self.dic)\n",
    "\n",
    "\n",
    "        #get the images\n",
    "        path0 = os.path.join(self.images_dir, name0)\n",
    "        img0 =  torch.load(path0)\n",
    "        path1 = os.path.join(self.images_dir, name1)\n",
    "        img1 = torch.load(path1)\n",
    "\n",
    "\n",
    "        if self.transfo is not None:\n",
    "            img0 = self.transfo(img0)\n",
    "            img1 = self.transfo(img1)\n",
    "\n",
    "        return img0, img1,  torch.from_numpy(np.array(label)).long(), name0, name1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcWzYRyzM1wb"
   },
   "source": [
    "**Q3** Why is the training accuracy lower than before? Compare the learning curves (validation accuracies) and the results on the test set. Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dafc8NUV7Gn"
   },
   "source": [
    "**Exercise #4** RankNet Loss (and ListNet Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrXFNbAtWqva"
   },
   "source": [
    "A milder version of the Hinge Loss has been widely used, particularly in search engine learning, known as the RankNet Loss.\n",
    "\n",
    "This cost function is derived from a parametric probabilistic model, the [Bradley-Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model).\n",
    "\n",
    "In a general version, it is assumed that the outcome of a comparison (or match) between two objects \"0\" and \"1\" (or two teams) is random and depends on real values associated with the objects (the \"team levels\") as follows:\n",
    "\\begin{align}\n",
    "P_0 = \\dfrac{f(y_0)}{f(y_0) + f(y_1)}\n",
    "\\tag{1}\n",
    "\\end{align}\n",
    "Where $P_0$ is the probability of choosing object \"0\" (or the first team winning), and $f$ is a strictly increasing function with positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnGSr-f0ZYUe"
   },
   "source": [
    "**Q1** In the case where $f(y) = e^{\\sigma y}$, what do the choice probabilities depend on? Write the log-likelihood of the event \"object $x$ is chosen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06sul9qLaLnt"
   },
   "source": [
    "**Q2** Derive an appropriate cost function for our ranking problem based on the given log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMHptL6PbLN3"
   },
   "source": [
    "**Q3** Implement and compare over twenty epochs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
