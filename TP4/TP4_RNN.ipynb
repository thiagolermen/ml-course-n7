{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "jWrcUhXzBBR2",
   "metadata": {
    "id": "jWrcUhXzBBR2"
   },
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "This module demonstrates the implementation of a simple RNN using PyTorch. It generates synthetic sequential data, trains the RNN model, and evaluates its predictions.\n",
    "\n",
    "Authors: Pierre Lepetit and Rachid El Montassir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9uYTfLUUozlC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uYTfLUUozlC",
    "outputId": "cbb4e22e-bf6a-4beb-ba94-6ceb2ba19d93"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "if os.path.exists(\"rnn.png\"):\n",
    "  ! git clone https://github.com/relmonta/ml-student.git\n",
    "  ! mv ml-student/TP4/*.png .\n",
    "  ! mv ml-student/TP4/*.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8qWseO4YohZp",
   "metadata": {
    "id": "8qWseO4YohZp"
   },
   "source": [
    "## What are RNNs?\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to process sequential data. Unlike traditional feedforward neural networks, which process the entire input independently, RNNs maintain a hidden state that captures information about the input seen so far. This hidden state is updated at each time step, allowing RNNs to retain information about the context of the sequence.\n",
    "\n",
    "RNNs are well-suited for tasks involving sequences, such as time series analysis, natural language processing, speech recognition, and more. They process inputs one step at a time, maintaining an internal state that captures information about the sequence.\n",
    "\n",
    "RNNs share the same set of parameters across all time steps. This parameter sharing allows the network to learn and generalize patterns from one part of the sequence to another. It is particularly useful when dealing with sequences of varying lengths.\n",
    "\n",
    "## Structure of an RNN block:\n",
    "\n",
    "The basic structure of an RNN block involves the following components:\n",
    "\n",
    "#### Input:\n",
    "\n",
    "- **X(t):** Input at time step t.\n",
    "\n",
    "#### Hidden State:\n",
    "\n",
    "- **H(t):** Hidden state at time step t.\n",
    "- **H(t-1):** Hidden state from the previous time step.\n",
    "\n",
    "The hidden state of an RNN block serves as its memory, allowing it to capture information from previous time steps and use it to influence future predictions.\n",
    "#### Output:\n",
    "\n",
    "- **H(t):** The module implemented in PyTorch considers the hidden state as the output at time step t, and also computed as follows :\n",
    "\n",
    "$$H(t) = tanh(W_{hh} \\cdot H(t-1) + W_{xh} \\cdot X(t) + b_h)$$\n",
    "\n",
    "where:\n",
    "- $(W_{hh})$ and $( W_{xh} )$ are weight matrices.\n",
    "- $(b_h)$ and $( b_y )$ are bias vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wpOAZnugopKy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "wpOAZnugopKy",
    "outputId": "0268678e-51d5-4e74-fcd2-09d27be30e10"
   },
   "outputs": [],
   "source": [
    "Image.open(\"rnn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6AAA4xh9otza",
   "metadata": {
    "id": "6AAA4xh9otza"
   },
   "source": [
    "**Note:** There are other implementations depending on the task and input features, and the output can be different from the hidden state.\n",
    "## RNN network:\n",
    "\n",
    "- **X:** The input sequence.\n",
    "- **H:** The concatenated hidden states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MemuEk4aovaS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "MemuEk4aovaS",
    "outputId": "e53f656e-067c-4b15-8e77-fd77f8eaede9"
   },
   "outputs": [],
   "source": [
    "Image.open(\"rnn_network.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pZs0f8DdlZb9",
   "metadata": {
    "id": "pZs0f8DdlZb9"
   },
   "source": [
    "## Challenges:\n",
    "\n",
    "While RNNs have proven effective for many sequence-based tasks, they suffer from challenges such as vanishing gradients and difficulties in capturing long-term dependencies. To address these issues, more advanced RNN architectures, such as Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), have been developed.\n",
    "\n",
    "In summary, Recurrent Neural Networks are powerful for modelling sequential data. Their ability to capture temporal dependencies makes them indispensable in applications ranging from natural language processing to time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db867df-4b5e-4980-9285-a84f9d379e6a",
   "metadata": {
    "id": "4db867df-4b5e-4980-9285-a84f9d379e6a"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yn-agbUVtLH0",
   "metadata": {
    "id": "yn-agbUVtLH0"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3496dfd-1e5e-425e-afaf-bb368db7c576",
   "metadata": {
    "id": "b3496dfd-1e5e-425e-afaf-bb368db7c576"
   },
   "source": [
    "# Generating synthetic sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677266ea-8739-4376-aa74-4626b8eeb46b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "677266ea-8739-4376-aa74-4626b8eeb46b",
    "outputId": "cfab24bb-35d4-487e-cfae-332b689f0a12"
   },
   "outputs": [],
   "source": [
    "noise_scale = 5e-2\n",
    "data = np.sin(np.arange(0, 200, 0.1)) + np.random.normal(0, noise_scale, 2000)\n",
    "split_at = 1000\n",
    "train_data = data[:split_at]\n",
    "test_data = data[split_at:]\n",
    "\n",
    "start_plot = 600\n",
    "stop_plot = 1200\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(range(start_plot, split_at),data[start_plot:split_at],label=\"train data\")\n",
    "plt.plot(range(split_at, stop_plot),test_data[:stop_plot-split_at],label=\"test data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Converting data to PyTorch tensor\n",
    "train_data = torch.FloatTensor(train_data).view(-1, 1)\n",
    "test_data = torch.FloatTensor(test_data).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I05dh_2GVLwp",
   "metadata": {
    "id": "I05dh_2GVLwp"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zPYvi48HVKy8",
   "metadata": {
    "id": "zPYvi48HVKy8"
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for the time series data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, input_sequence_len, output_size=1):\n",
    "        self.data = data\n",
    "        self.input_sequence_len = input_sequence_len\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_sequence_len - self.output_size\n",
    "\n",
    "    def __getitem__(self, start_idx):\n",
    "        # Extracting a sequence of data\n",
    "        stop_idx = start_idx + self.input_sequence_len\n",
    "        sequence = self.data[start_idx:stop_idx]\n",
    "        # Target is the next value in the sequence\n",
    "        target = self.data[stop_idx:stop_idx + self.output_size]\n",
    "        return {'sequence': sequence, 'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDCF_eJFUsp-",
   "metadata": {
    "id": "rDCF_eJFUsp-"
   },
   "source": [
    "# Simple Fully Connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TwJNhjalX3nd",
   "metadata": {
    "id": "TwJNhjalX3nd"
   },
   "source": [
    "**Exercise 1**: We start by implement a simple fully connected model.\n",
    "\n",
    "Implement a fully connected model using an input layer and a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vZ61oRG3UxIP",
   "metadata": {
    "deletable": false,
    "id": "vZ61oRG3UxIP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "018ea46890a5dd34be4591e7b0b59608",
     "grade": false,
     "grade_id": "cell-eb8bbf9d2dd87ad5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleFC(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleFC, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x.squeeze(-1))\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3etV0iVXVbOr",
   "metadata": {
    "id": "3etV0iVXVbOr"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zipytPgxYoTF",
   "metadata": {
    "id": "zipytPgxYoTF"
   },
   "source": [
    "**Exercise 2**: Implement the train loop (use GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rwsT16M0VFCT",
   "metadata": {
    "deletable": false,
    "id": "rwsT16M0VFCT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9e08ccd0e43dc66fe4ad181e1b2bff0",
     "grade": false,
     "grade_id": "cell-3ee814140f3d3e52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "def train(model, dataloader, learning_rate=0.001, epochs=100, device=device):\n",
    "    \"\"\"\n",
    "    Trains a Recurrent model.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): model to be trained.\n",
    "    - dataloader (DataLoader): DataLoader for the training data.\n",
    "    - learning_rate (float): Learning rate, default to 1e-3.\n",
    "    - epochs (int): Number of training epochs.\n",
    "    - device (str): Device to use ('cuda' for GPU, 'cpu' for CPU).\n",
    "\n",
    "    Returns:\n",
    "    - model (nn.Module): Trained model.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    since = time.time()\n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = 0\n",
    "        # for batch in dataloader:\n",
    "        #     ....\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        losses.append(epoch_loss / len(dataloader))\n",
    "    print(f\"\\nTotal time :{(time.time()-since):.1f} s\")\n",
    "    return model,losses\n",
    "\n",
    "def plot_loss(losses):\n",
    "  plt.figure(figsize=(14, 5))\n",
    "  plt.plot(range(2, len(losses) + 1), losses[1:],linewidth=2, color='b')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.title('Model Losses Over Epochs')\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tjJyW6rmVhOk",
   "metadata": {
    "id": "tjJyW6rmVhOk"
   },
   "outputs": [],
   "source": [
    "input_sequence_len = 20\n",
    "output_size = 3\n",
    "\n",
    "dataset = TimeSeriesDataset(train_data, input_sequence_len, output_size=output_size)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_D81DloRVpBs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "_D81DloRVpBs",
    "outputId": "979790ff-5f98-484c-b4eb-f1f14c4e82d7"
   },
   "outputs": [],
   "source": [
    "# Plot a sample sequence and target from the DataLoader\n",
    "sample_batch = next(iter(dataloader))\n",
    "sample_sequence = sample_batch['sequence'][0].numpy()\n",
    "sample_target = sample_batch['target'][0].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sample_sequence, linewidth=2, label='Sample Sequence')\n",
    "plt.scatter(input_sequence_len + 0, sample_target[0], color='red', label='Target sequence')\n",
    "for i in range(1,output_size):\n",
    "    plt.scatter(input_sequence_len + i, sample_target[i], color='red')\n",
    "plt.legend()\n",
    "plt.title('Sample Sequence and Target from DataLoader')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IxX_ORK_WQDd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "IxX_ORK_WQDd",
    "outputId": "c4945ff0-78ee-4db7-a003-275e680e0178"
   },
   "outputs": [],
   "source": [
    "input_size = 20\n",
    "hidden_size = 512\n",
    "epochs = 100\n",
    "\n",
    "fc_model = SimpleFC(input_size, hidden_size, output_size)\n",
    "\n",
    "trained_fc_model,fc_losses = train(fc_model, dataloader, learning_rate=0.01, epochs=epochs)\n",
    "plot_loss(fc_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ijVoJV7gJ2Et",
   "metadata": {
    "id": "ijVoJV7gJ2Et"
   },
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fb90pGrHpIF2",
   "metadata": {
    "id": "Fb90pGrHpIF2"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, input_sequence_len, future_steps,output_size, device=device):\n",
    "    \"\"\"\n",
    "    Evaluates the given model by making predictions for future time steps.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): Trained model to be evaluated.\n",
    "    - data (torch.Tensor): Input data for evaluation.\n",
    "    - input_sequence_len (int): Length of the input sequence.\n",
    "    - future_steps (int): Number of future time steps to predict.\n",
    "    - device (str): Device to use ('cuda' for GPU, 'cpu' for CPU).\n",
    "\n",
    "    Returns:\n",
    "    - predictions (torch.Tensor): Predicted values for future time steps.\n",
    "    - errors (torch.Tensor): Errors for each time step.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Generating a sequence for evaluation\n",
    "        eval_sequence = data[-input_sequence_len:].view(1, -1, 1).to(device)\n",
    "        # Making predictions for the future time steps\n",
    "        predictions = eval_sequence.clone().squeeze(-1)\n",
    "        for i in range(0,future_steps,output_size):\n",
    "            pred = model(eval_sequence[:, i:, :])\n",
    "            eval_sequence = torch.cat([eval_sequence, pred.unsqueeze(-1)], dim=1)\n",
    "            predictions = torch.cat([predictions, pred], dim=1)\n",
    "\n",
    "        predictions = predictions.cpu()\n",
    "        print(predictions.shape)\n",
    "        print(test_data.shape)\n",
    "        errors = (predictions[:,input_sequence_len:input_sequence_len +\n",
    "                              future_steps].view(-1) - test_data[:future_steps].view(-1))**2\n",
    "\n",
    "    return predictions[:,input_sequence_len:input_sequence_len + future_steps], errors.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gqhp-W1IWCI3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqhp-W1IWCI3",
    "outputId": "5cfaf23b-4d54-4edb-97c0-aa82c7872fc8"
   },
   "outputs": [],
   "source": [
    "future_steps = 200\n",
    "fc_predictions, fc_errors = evaluate_model(trained_fc_model, train_data, input_sequence_len, future_steps,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cwxCkmJ-6Hu",
   "metadata": {
    "id": "7cwxCkmJ-6Hu"
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "def plot_predictions(predictions,title):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(np.arange(start_plot,len(train_data)), train_data[start_plot:].numpy(), 'g--',label='Training data',linewidth=2)\n",
    "    plt.plot(np.arange(len(train_data) - input_sequence_len, len(train_data)), train_data[-input_sequence_len:].numpy(),'r--',label='Input sequence',linewidth=2)\n",
    "    plt.plot(np.arange(len(train_data), len(train_data) + future_steps), test_data[:future_steps].view(-1).numpy(),'--', label='Test sequence',linewidth=2)\n",
    "    plt.plot(np.arange(len(train_data), len(train_data) + future_steps), predictions.view(-1).numpy(),'orange', label='Predicted sequence',linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZjTOknC8W6-k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "ZjTOknC8W6-k",
    "outputId": "aa32af4e-dedb-4da9-82a9-267e781337b1"
   },
   "outputs": [],
   "source": [
    "plot_predictions(fc_predictions,'Simple FC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855a17d-c0f9-489d-9723-cc0fd28c5b01",
   "metadata": {
    "id": "f855a17d-c0f9-489d-9723-cc0fd28c5b01"
   },
   "source": [
    "# Setting up the RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xlgWkiWWaBNd",
   "metadata": {
    "id": "xlgWkiWWaBNd"
   },
   "source": [
    "**Exercise 3**: Replace the dense layer used in the last model by an RNN layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d0322-4ad5-4338-9f2b-452fe44b5575",
   "metadata": {
    "deletable": false,
    "id": "ff9d0322-4ad5-4338-9f2b-452fe44b5575",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa945f7cfabf18b508f168cc7446414b",
     "grade": false,
     "grade_id": "cell-ff9e5be41feffe82",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Recurrent Neural Network (RNN) model class.\n",
    "\n",
    "    Args:\n",
    "    - input_size (int): Number of input features.\n",
    "    - hidden_size (int): Number of hidden units in the RNN layer.\n",
    "    - output_size (int): Number of output features.\n",
    "\n",
    "    Attributes:\n",
    "    - rnn (nn.RNN): RNN layer.\n",
    "    - fc (nn.Linear): Fully connected layer for output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efDgLC-RB1dz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "efDgLC-RB1dz",
    "outputId": "9dcb9cf1-c961-4316-bcce-ee672c17af59"
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 32\n",
    "\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "trained_rnn_model,rnn_losses = train(rnn_model, dataloader, learning_rate=0.01, epochs=epochs)\n",
    "plot_loss(rnn_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RXImrgSipi3X",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXImrgSipi3X",
    "outputId": "308ca132-d950-4834-ccbc-74c66147674a"
   },
   "outputs": [],
   "source": [
    "future_steps = 200\n",
    "rnn_predictions, rnn_errors = evaluate_model(trained_rnn_model, train_data, input_sequence_len, future_steps,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Se-WI1-hETEv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "Se-WI1-hETEv",
    "outputId": "0fef940c-cb4d-4b75-c420-8bf6ff5b82a6"
   },
   "outputs": [],
   "source": [
    "plot_predictions(rnn_predictions,'Simple RNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OIRZ0gu2oZDs",
   "metadata": {
    "id": "OIRZ0gu2oZDs"
   },
   "source": [
    "# Long Short-Term Memory (LSTM)\n",
    "In RNNs, the hidden state acts as a memory, but it tends to lose information over time due to the recurrent nature of the network.\n",
    "LSTMs introduce a memory cell that can store and retrieve information over long periods. The cell state allows LSTMs to maintain a more stable memory of past inputs.\n",
    "\n",
    "Standard RNNs suffer from the vanishing and exploding gradient problems, which make it difficult for them to capture and propagate information across long sequences. As the sequence length increases, RNNs struggle to remember information from earlier time steps.\n",
    "LSTMs were specifically designed to address the vanishing gradient problem. The architecture uses memory cell and gating mechanisms that allow it to selectively remember or forget information over long sequences. This enables LSTMs to capture dependencies over extended time horizons compared to RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M_Dx3sDPpu5L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "M_Dx3sDPpu5L",
    "outputId": "c718856e-100d-4119-af05-07a961438a60"
   },
   "outputs": [],
   "source": [
    "Image.open(\"lstm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kL-xiOSkpvRS",
   "metadata": {
    "id": "kL-xiOSkpvRS"
   },
   "source": [
    "Here, we introduce LSTM, which is a type of recurrent neural network architecture designed to overcome the vanishing gradient problem and capture long-term dependencies in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C1kYpAnta2se",
   "metadata": {
    "id": "C1kYpAnta2se"
   },
   "source": [
    "**Exercise 4**: Replace the RNN layer by an LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WLpNi6QqLN78",
   "metadata": {
    "deletable": false,
    "id": "WLpNi6QqLN78",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21f8c8cbb8e9a0a6bc958d9af045f7d",
     "grade": false,
     "grade_id": "cell-c62011d16c3ca68e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Long Short-Term Memory (LSTM) model class.\n",
    "\n",
    "    Args:\n",
    "    - input_size (int): Number of input features.\n",
    "    - hidden_size (int): Number of hidden units in the LSTM layer.\n",
    "    - output_size (int): Number of output features.\n",
    "\n",
    "    Attributes:\n",
    "    - lstm (nn.LSTM): LSTM layer.\n",
    "    - fc (nn.Linear): Fully connected layer for output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AwtWKapCoivI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "AwtWKapCoivI",
    "outputId": "8c204c82-0f11-45a3-fc53-4195a73ae6c2"
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 32\n",
    "\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "\n",
    "trained_lstm_model,lstm_losses = train(lstm_model, dataloader, learning_rate=0.01, epochs=epochs)\n",
    "plot_loss(lstm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o-QB6W9IoxHp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "o-QB6W9IoxHp",
    "outputId": "a7693717-1c22-4c9b-8ad9-12fe81831595"
   },
   "outputs": [],
   "source": [
    "lstm_predictions, lstm_errors = evaluate_model(trained_lstm_model, train_data, input_sequence_len, future_steps,output_size)\n",
    "plot_predictions(lstm_predictions,'Simple LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GZOxFYGMbI5K",
   "metadata": {
    "id": "GZOxFYGMbI5K"
   },
   "source": [
    "**Exercise 5**: Use multiple LSTM layer (use `num_layers` option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dUbVweMSqff5",
   "metadata": {
    "deletable": false,
    "id": "dUbVweMSqff5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a772fc29f265aa0e053bf4cf4d73958a",
     "grade": false,
     "grade_id": "cell-8565d8f860972cb5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImprovedLSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved Long Short-Term Memory (LSTM) model class.\n",
    "\n",
    "    Args:\n",
    "    - input_size (int): Number of input features.\n",
    "    - hidden_size (int): Number of hidden units in the LSTM layer.\n",
    "    - output_size (int): Number of output features.\n",
    "    - num_layers (int, optional): Number of LSTM layers. Default is 1.\n",
    "\n",
    "    Attributes:\n",
    "    - lstm (nn.LSTM): LSTM layer.\n",
    "    - relu (nn.ReLU): ReLU activation function.\n",
    "    - dropout (nn.Dropout): Dropout layer.\n",
    "    - fc (nn.Linear): Fully connected layer for output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,dropout=0.0):\n",
    "        super(ImprovedLSTMModel, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9r-6VppmEXq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "f9r-6VppmEXq",
    "outputId": "89c71611-3ee8-4fff-fc97-6114b19de943"
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 32\n",
    "num_layers=5\n",
    "\n",
    "lstm_model_2 = ImprovedLSTMModel(input_size, hidden_size, output_size,num_layers=num_layers)\n",
    "\n",
    "trained_lstm_model_2,improved_lstm_losses = train(lstm_model_2, dataloader, learning_rate=0.01, epochs=epochs)\n",
    "plot_loss(improved_lstm_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ZgcIODmYdF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "00ZgcIODmYdF",
    "outputId": "aa85816c-c535-4e5b-d4e4-1970b4a5ea83"
   },
   "outputs": [],
   "source": [
    "lstm_predictions_2, lstm_errors_2 = evaluate_model(trained_lstm_model_2, train_data, input_sequence_len, future_steps, output_size)\n",
    "plot_predictions(lstm_predictions_2,'Improved LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wsmeWtPF_Jk1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "wsmeWtPF_Jk1",
    "outputId": "d884d76b-ba82-4b7e-8e59-330475181de1"
   },
   "outputs": [],
   "source": [
    "# Prediction errors comparison\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(range(len(train_data),len(train_data)+future_steps),rnn_errors,\n",
    "         label=f\"Simple FC | Mean error : {np.mean(fc_errors):.4f}\",linewidth=2)\n",
    "plt.plot(range(len(train_data),len(train_data)+future_steps),rnn_errors,\n",
    "         label=f\"Simple RNN | Mean error : {np.mean(rnn_errors):.4f}\",linewidth=2)\n",
    "plt.plot(range(len(train_data),len(train_data)+future_steps),lstm_errors,\n",
    "         label=f\"Simple LSTM | Mean error : {np.mean(lstm_errors):.4f}\",linewidth=2)\n",
    "plt.plot(range(len(train_data),len(train_data)+future_steps),lstm_errors_2,\n",
    "         label=f\"Improved LSTM | Mean error : {np.mean(lstm_errors_2):.4f}\",linewidth=2)\n",
    "plt.legend()\n",
    "plt.title(\"Prediction errors\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"$(y-\\hat(y))^2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sZqD0-vn9VjA",
   "metadata": {
    "id": "sZqD0-vn9VjA"
   },
   "source": [
    "# Training using real temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jPpBLF0REvX_",
   "metadata": {
    "id": "jPpBLF0REvX_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"/content/temperatures.csv\"\n",
    "df = pd.read_csv(file_path, header=None, sep=\" \")\n",
    "df = df.rename(columns={df.columns[0]: 'Date',df.columns[-1]: 'Temperature'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CKzXpItL59SG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "CKzXpItL59SG",
    "outputId": "5a6592d4-23cf-4678-f3b0-e087aa2b4488"
   },
   "outputs": [],
   "source": [
    "real_data = np.array(df.Temperature)-273.15 # − 273.15 to go from K to °C\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(real_data)\n",
    "plt.xlabel(\"Time steps (in hours)\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gp4x0bjI2Inp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "Gp4x0bjI2Inp",
    "outputId": "216af9c9-48e5-4504-8763-bd9e89fc1953"
   },
   "outputs": [],
   "source": [
    "split_at = 1900\n",
    "train_data = real_data[:split_at]\n",
    "test_data = real_data[split_at:]\n",
    "\n",
    "start_plot = 1700\n",
    "stop_plot = len(real_data)\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(range(start_plot, split_at),real_data[start_plot:split_at],label=\"train data\")\n",
    "plt.plot(range(split_at, stop_plot),test_data[:stop_plot-split_at],label=\"test data\")\n",
    "plt.xlabel(\"Time steps (in hours)\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Converting data to PyTorch tensor\n",
    "train_data = torch.FloatTensor(train_data).view(-1, 1)\n",
    "test_data = torch.FloatTensor(test_data).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3vIW07DJcbJ_",
   "metadata": {
    "id": "3vIW07DJcbJ_"
   },
   "source": [
    "**Exercise 6**: Implement a model to predict the temperature over the 24 hours using the last 48 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZkpv5wm6Frm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "deletable": false,
    "id": "AZkpv5wm6Frm",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78bd4b78c7f251b945c45969189c0b0c",
     "grade": false,
     "grade_id": "cell-826a173d85267a64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "e3262e7b-8a57-4151-95ee-4f3a6dca4d2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "input_sequence_len = 2*24 # Take the last 72 hours as an input sequence\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ugxden27o2x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "deletable": false,
    "id": "1ugxden27o2x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59f87c5a1761e01da5ec21858cd6fcc7",
     "grade": false,
     "grade_id": "cell-19b976b2c3ff6166",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "d328962d-a975-4147-d962-a17541ddbc5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "future_steps = 2*24 # A prediction over the next 24 hours\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zTlE26VZBxhy",
   "metadata": {
    "id": "zTlE26VZBxhy"
   },
   "source": [
    "## Vanishing gradients\n",
    "Vanishing gradients is a problem that can occur during the training of deep neural networks, particularly in recurrent neural networks (RNNs) and networks with many layers. It refers to the situation where the gradients of the loss function with respect to the parameters become extremely small, causing the weights to stop updating or updating very slowly during the training process.\n",
    "\n",
    "The issue arises during the backpropagation algorithm, which is used to update the weights of the neural network based on the computed gradients. When the gradients become very small, the updates to the weights also become very small, and as a result, the weights may not change significantly, or they may converge to values where the network does not learn effectively.\n",
    "\n",
    "Let's consider a simple feedforward neural network with multiple layers.\n",
    "\n",
    "In a neural network, the weights are updated using the gradient descent optimization algorithm. The weights are adjusted in the opposite direction of the gradient of the loss function with respect to the weights. Mathematically, this can be expressed as:\n",
    "\n",
    "$$ W_{ij}^{(l)} = W_{ij}^{(l)} - \\alpha \\frac{\\partial L}{\\partial W_{ij}^{(l)}} $$\n",
    "\n",
    "where:\n",
    "- $( W_{ij}^{(l)} )$ is the weight connecting neuron $(i)$ in layer $(l-1)$ to neuron $(j)$ in layer $(l)$.\n",
    "- $( \\alpha )$ is the learning rate.\n",
    "- $( L )$ is the loss function.\n",
    "\n",
    "The key factor that leads to vanishing gradients is the chain rule of calculus applied during backpropagation. The gradient of the loss with respect to the weights in layer $(l)$ is computed by recursively applying the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial W_{ij}^{(l)}} $$\n",
    "\n",
    "where:\n",
    "- $( a_j^{(l)} )$ is the input to neuron $(j)$ in layer $(l)$.\n",
    "\n",
    "The vanishing gradient problem arises when the terms $( \\frac{\\partial L}{\\partial a_j^{(l)}} )$ and $( \\frac{\\partial a_j^{(l)}}{\\partial W_{ij}^{(l)}} )$ become very small for deep layers during the multiplication process.\n",
    "\n",
    "\n",
    "Sometimes, this issue occurs with certain activation functions, such as the sigmoid or hyperbolic tangent (tanh), have saturating regions where the gradients become very small. In these regions, the derivatives of the functions approach zero, causing the gradients to vanish.\n",
    "This happens due to the use of activation functions that saturate, such as the sigmoid or hyperbolic tangent (tanh).\n",
    "\n",
    "Let's consider the sigmoid activation function as an example:\n",
    "\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "The derivative of the sigmoid function is:\n",
    "\n",
    "$$ \\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) $$\n",
    "\n",
    "In the context of the neural network formulation, if $( a_j^{(l)} )$ is large (positive or negative), the sigmoid function saturates, and its derivative becomes very close to zero. This small derivative gets multiplied with other small derivatives as we move backward through the layers during backpropagation, causing the overall gradient to vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb830b-3881-4ab0-a03e-1bfc7a08740e",
   "metadata": {
    "id": "f3cb830b-3881-4ab0-a03e-1bfc7a08740e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
